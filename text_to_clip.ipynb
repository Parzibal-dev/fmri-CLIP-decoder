{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bac71f2",
   "metadata": {},
   "source": [
    "### Text to CLIP cross modality ###\n",
    "This notebook is the implementation of our original hypothesis <br>\n",
    "Before starting the open ended project, we have hypothesized that meaning of textual data (both individual words of concepts and full sentences) and images of the same concepts might be interpreted in the same places inside the brain. <br>\n",
    "To try proving said hypothesis, we set out to expand on the work done by Pereira et al. (2018). In their work, \"Toward a universal decoder of linguistic meaning from brain activation\", Pereira et al. have made big strides in proving that meaning of different concepts, ranging from abstract to physical objects, is being parsed withing the brain in the same area. They scanned the"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740ac1ec",
   "metadata": {},
   "source": [
    "#### Setup ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2723a1f8",
   "metadata": {},
   "source": [
    "##### Dependecies #####\n",
    "First, let's download all relevant dependecies to check our hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95771580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ftfy in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (6.3.1)\n",
      "Requirement already satisfied: regex in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (2025.7.34)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (2.3.2)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.5-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ftfy) (0.2.13)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.59.0-cp312-cp312-win_amd64.whl.metadata (110 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.8-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.5-cp312-cp312-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 1.8/8.1 MB 12.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 27.9 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.3-cp312-cp312-win_amd64.whl (226 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.59.0-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   -------------------------------- ------- 1.8/2.2 MB 98.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.1/2.2 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 4.4 MB/s eta 0:00:00\n",
      "Using cached kiwisolver-1.4.8-cp312-cp312-win_amd64.whl (71 kB)\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\n",
      "   ---------------------------------------- 0/6 [pyparsing]\n",
      "   ---------------------------------------- 0/6 [pyparsing]\n",
      "   ------ --------------------------------- 1/6 [kiwisolver]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   -------------------------- ------------- 4/6 [contourpy]\n",
      "   -------------------------- ------------- 4/6 [contourpy]\n",
      "   -------------------------- ------------- 4/6 [contourpy]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   ---------------------------------------- 6/6 [matplotlib]\n",
      "\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.59.0 kiwisolver-1.4.8 matplotlib-3.10.5 pyparsing-3.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gdown in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (5.2.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from gdown) (4.13.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from gdown) (3.18.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from gdown) (2.32.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from beautifulsoup4->gdown) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from beautifulsoup4->gdown) (4.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from requests[socks]->gdown) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from requests[socks]->gdown) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from requests[socks]->gdown) (2025.8.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->gdown) (0.4.6)\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\user\\appdata\\local\\temp\\pip-req-build-z_yhlwzd\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: ftfy in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from clip==1.0) (6.3.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from clip==1.0) (24.2)\n",
      "Requirement already satisfied: regex in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from clip==1.0) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from clip==1.0) (4.67.1)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from clip==1.0) (2.7.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from clip==1.0) (0.22.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (2025.7.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torchvision->clip==1.0) (2.3.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torchvision->clip==1.0) (11.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\user\\AppData\\Local\\Temp\\pip-req-build-z_yhlwzd'\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%pip install ftfy regex tqdm scikit-learn numpy matplotlib\n",
    "%pip install -U gdown\n",
    "%pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32aa891",
   "metadata": {},
   "source": [
    "##### Data #####\n",
    "Now, we can import all of our relevant data! <br>\n",
    "For this project, we've created a drive folder, containing all of the relevant code and data from the original Pereira et al. paper. Moreover, because the list of concepts and the related images from the original paper is static, we pre-calculated all of the relevant CLIP embeddings (The exact code we used can be seen here in \"one_time_drive_setup\"), and persisted them to drive. Let's download all of the relevant data, so we could continue our analysis. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8345d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "# If the data already exists, we don't need to download it again\n",
    "if not Path(\"data\").exists():\n",
    "    # Check operating system - handlind the data is done differently on Windows and Linux.\n",
    "    # This will allow us to run the code on Colab, locally, and on any other platform we may choose.\n",
    "    IS_WINDOWS = platform.system() == \"Windows\"\n",
    "    \n",
    "    if IS_WINDOWS:\n",
    "        !python -m gdown --folder --id 1CwmFOsYFnq6t33KAzpvw0gaOTQXbcozs -O ./data/\n",
    "        !powershell -NoProfile -Command \"Expand-Archive -Path ./data/experiment-images.zip -DestinationPath ./data/ -Force\"\n",
    "        !powershell -NoProfile -Command \"Remove-Item ./data/experiment-images.zip\"\n",
    "    else:\n",
    "        !gdown --folder --id 1CwmFOsYFnq6t33KAzpvw0gaOTQXbcozs --output ./data\n",
    "        !unzip ./data/experiment-images.zip\n",
    "        !rm ./data/experiment-images.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f59534",
   "metadata": {},
   "source": [
    "For the embeddings, we saved the following:\n",
    "* <u>Textual data</u> - we saved the embedding of the prompt \"A picture of {c}\", where c is the name of the relevant concept. The reason for that choice is that CLIP reacts very well to prompting, and that embedding is improving results over the non-prompt version.\n",
    "* <u>Visual data</u> - because each concept has 6 separate images describing it, we embedded all of them to CLIP's embedding space. Since this is not a singular vector, there are several approaches on how to parse the data, but the base embedding will be the same nonetheless, so we saved the embeddings for all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc8be26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with np.load(\"data/clip_text_embeddings.npz\") as text_embeddings:\n",
    "    clip_text_embeddings = text_embeddings[\"data\"]\n",
    "\n",
    "with np.load(\"data/clip_image_embeddings.npz\") as image_embeddings:\n",
    "    clip_image_embeddings = image_embeddings[\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fbdfa0",
   "metadata": {},
   "source": [
    "#### Training ####\n",
    "As stated earlier, we are going to keep Pereira et al's method of learning a **linear** decoder, that would be able to generalize from the data it saw to unseen data, and thus capture deeper meaning than the training data itself. In the original paper, they managed to achieve results which are **much** better than chances, implying that a linear decoder is more than sufficient to capture meaning of textual-fMRI data when the data is projected onto an embedding space. <br>\n",
    "We have changed the embedding space from GloVe to CLIP, in order to see if textual-fMRI data can capture meaning of images as well, but we believe that adding extra complexity to the model will defeat our purpose. If our hypothesis is correct, then a linear decoder will be able to capture the meaning of the images in the same embedding space, without adding any non-linearity - a simple model should suffice, as much as is did for the same modality data. <br>\n",
    "For that reason, we are going to train our decoder in the same way that it was trained in the original paper:\n",
    "* For each participant's fMRI data - we're only going to take the top 5000 relevant `voxels`.\n",
    "* For the decoder - we're going to learn a simple ridge regression (using the same code)\n",
    "<br> \n",
    "Another important thing to remember is that we're only going to feed the function **textual** fMRI data, because our hypothesis states that from the same areas in the brain responsible for interpreting textual data can also give us insight on visual data. That means that we will only train our model on textual data, and withold any visual data for evaluation only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174aad21",
   "metadata": {},
   "source": [
    "First, let's define the function that will help us determine the top 5000 relevant voxels: <br>\n",
    "The fMRI data from the experiments consists of a big series of voxel each corresponding to the activation in a different area in the brain. The problem here is that most of the voxels are non-importent and are just noise which will reduce our model's accuracy. For that reason we will clean up the data and only use the 5000 most influencing voxels out of the 200,000 in the original fMRI data and we will be doing so using the select_top_voxels_indexes function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3bf99d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression\n",
    "import numpy as np\n",
    "\n",
    "def select_top_voxels_indices(fmri_data, semantic_vectors, num_voxels=5000):\n",
    "    f_scores = []\n",
    "    for i in range(semantic_vectors.shape[1]):\n",
    "        f, _ = f_regression(fmri_data, semantic_vectors[:, i])\n",
    "        f_scores.append(f)\n",
    "\n",
    "    f_scores = np.array(f_scores)\n",
    "    voxel_scores = np.max(f_scores, axis=0)\n",
    "    top_voxel_indices = np.argsort(voxel_scores)[-num_voxels:]\n",
    "\n",
    "    return top_voxel_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4ef844",
   "metadata": {},
   "source": [
    "Now, let's take the fMRI textual data from our data folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0bc4f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "mat = scipy.io.loadmat(\"data/brain-responses-data/examples_180concepts_wordclouds.mat\")\n",
    "fmri_text_data = mat[\"examples\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f5f9a0",
   "metadata": {},
   "source": [
    "and get only our top voxels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7b9fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_voxel_indices = select_top_voxels_indices(fmri_text_data, clip_text_embeddings)\n",
    "reduced_fmri_data = fmri_text_data[:, top_voxel_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815f6c1f",
   "metadata": {},
   "source": [
    "Finally, let's train our textual decoder. We'll use the function \"learn_decoder\", which we took directly from the original paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc27aa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" learn_decoder \"\"\"\n",
    "import sklearn.linear_model\n",
    "\n",
    "def learn_decoder(data, vectors):\n",
    "     \"\"\" Given data (a CxV matrix of V voxel activations per C concepts)\n",
    "     and vectors (a CxD matrix of D semantic dimensions per C concepts)\n",
    "     find a matrix M such that the dot product of M and a V-dimensional \n",
    "     data vector gives a D-dimensional decoded semantic vector. \n",
    "\n",
    "     The matrix M is learned using ridge regression:\n",
    "     https://en.wikipedia.org/wiki/Tikhonov_regularization\n",
    "     \"\"\"\n",
    "     ridge = sklearn.linear_model.RidgeCV(\n",
    "         alphas=[1, 10, .01, 100, .001, 1000, .0001, 10000, .00001, 100000, .000001, 1000000],\n",
    "         fit_intercept=False\n",
    "     )\n",
    "     ridge.fit(data, vectors)\n",
    "     return ridge.coef_.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f44bc1b",
   "metadata": {},
   "source": [
    "The training itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b7d1741",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = learn_decoder(reduced_fmri_data, clip_text_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2901909",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "For evaluation, we're going to have the following guiding principles:\n",
    "1. \"Distance\" between datapoints - \n",
    "\n",
    "\n",
    "1. Strategy - because quality fMRI data is extremely limited (Pereira is still the only open English dataset for fMRI single conept data, similary to what we're trying to model. There are some other alternatives - Allen 672 which is Chinese, Tuckute 2024, which is for full sentences only, six words each, each of them on no more than 16 participants), we don't have much data to work with. That means we want to base our decoder on all of the available data, scarce as it is. Because of that fact, we'll choose to evaluate the data using K-Fold Cross Validation - that way, we can train on every piece of data we have, and still evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dc3836",
   "metadata": {},
   "source": [
    "### Multimodal to Cross-Modality\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
