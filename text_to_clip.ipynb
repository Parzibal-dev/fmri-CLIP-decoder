{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bac71f2",
   "metadata": {},
   "source": [
    "### Text to CLIP cross modality ###\n",
    "This notebook is the implementation of our original hypothesis <br>\n",
    "Before starting the open ended project, we have hypothesized that meaning of textual data (both individual words of concepts and full sentences) and images of the same concepts might be interpreted in the same places inside the brain. <br>\n",
    "To try proving said hypothesis, we set out to expand on the work done by Pereira et al. (2018). In their work, \"Toward a universal decoder of linguistic meaning from brain activation\", Pereira et al. have made big strides in proving that meaning of different concepts, ranging from abstract to physical objects, is being parsed withing the brain in the same area. They scanned the"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740ac1ec",
   "metadata": {},
   "source": [
    "#### Setup ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2723a1f8",
   "metadata": {},
   "source": [
    "##### Dependecies #####\n",
    "First, let's download all relevant dependecies to check our hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95771580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ftfy in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (6.3.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: regex in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (2025.7.34)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ftfy) (0.2.13)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: gdown in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from gdown) (4.13.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from gdown) (3.18.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from gdown) (2.32.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from beautifulsoup4->gdown) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from beautifulsoup4->gdown) (4.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from requests[socks]->gdown) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from requests[socks]->gdown) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from requests[socks]->gdown) (2025.8.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->gdown) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\user\\appdata\\local\\temp\\pip-req-build-q39rohl3\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: ftfy in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from clip==1.0) (6.3.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from clip==1.0) (24.2)\n",
      "Requirement already satisfied: regex in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from clip==1.0) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from clip==1.0) (4.67.1)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from clip==1.0) (2.7.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from clip==1.0) (0.22.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (2025.7.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torchvision->clip==1.0) (2.3.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torchvision->clip==1.0) (11.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\user\\AppData\\Local\\Temp\\pip-req-build-q39rohl3'\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%pip install ftfy regex tqdm\n",
    "%pip install -U gdown\n",
    "%pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32aa891",
   "metadata": {},
   "source": [
    "##### Data #####\n",
    "Now, we can import all of our relevant data! <br>\n",
    "For this project, we've created a drive folder, containing all of the relevant code and data from the original Pereira et al. paper. Moreover, because the list of concepts and the related images from the original paper is static, we pre-calculated all of the relevant CLIP embeddings (The exact code we used can be seen here in \"one_time_drive_setup\"), and persisted them to drive. Let's download all of the relevant data, so we could continue our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8345d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "# If the data already exists, we don't need to download it again\n",
    "if not Path(\"data\").exists():\n",
    "    # Check operating system - handlind the data is done differently on Windows and Linux.\n",
    "    # This will allow us to run the code on Colab, locally, and on any other platform we may choose.\n",
    "    IS_WINDOWS = platform.system() == \"Windows\"\n",
    "    \n",
    "    if IS_WINDOWS:\n",
    "        !python -m gdown --folder --id 1CwmFOsYFnq6t33KAzpvw0gaOTQXbcozs -O ./data/\n",
    "        !powershell -NoProfile -Command \"Expand-Archive -Path ./data/experiment-images.zip -DestinationPath ./data/ -Force\"\n",
    "        !powershell -NoProfile -Command \"Remove-Item ./data/experiment-images.zip\"\n",
    "    else:\n",
    "        !gdown --folder --id 1CwmFOsYFnq6t33KAzpvw0gaOTQXbcozs --output ./data\n",
    "        !unzip ./data/experiment-images.zip\n",
    "        !rm ./data/experiment-images.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fbdfa0",
   "metadata": {},
   "source": [
    "#### Training ####\n",
    "As stated earlier, we are going to keep Pereira et al's method of learning a **linear** decoder, that would be able to generalize from the data it saw to unseen data, and thus capture deeper meaning than the training data itself. In the original paper, they managed to achieve results which are **much** better than chances, implying that a linear decoder is more than sufficient to capture meaning of textual-fMRI data when the data is projected onto an embedding space. <br>\n",
    "We have changed the embedding space from GloVe to CLIP, in order to see if textual-fMRI data can capture meaning of images as well, but we believe that adding extra complexity to the model will defeat our purpose. If our hypothesis is correct, then a linear decoder will be able to capture the meaning of the images in the same embedding space, without adding any non-linearity - a simple model should suffice, as much as is did for the same modality data. <br>\n",
    "For that reason, we are going to train our decoder in the same way that it was trained in the original paper:\n",
    "* For each participant's fMRI data - we're only going to take the top 5000 relevant `voxels`.\n",
    "* For the decoder - we're going to learn a simple ridge regression (using the same code)\n",
    "<br> \n",
    "Another important thing to remember is that we're only going to feed the function **textual** fMRI data, because our hypothesis states that from the same areas in the brain responsible for interpreting textual data can also give us insight on visual data. That means that we will only train our model on textual data, and withold any visual data for evaluation only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2901909",
   "metadata": {},
   "source": [
    "#### Evaluation ####\n",
    "For evaluation, we're going to stick to the original paper's way of ranking.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
