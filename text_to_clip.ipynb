{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfccc9d6",
   "metadata": {},
   "source": [
    "# DecVTC - Voxels To CLIP Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546593e2",
   "metadata": {},
   "source": [
    "Over the last 15 years, there has been an exponentially compounding effort of interpreting the 'Meaning' of language by machines. With the astronomical rise of LLMs, fueled by the giant leap of improvement in attention mechanisms, understanding how meaning of input is parsed and understood is the by far most researched topic in the Data Science world, and particularly in the NLP world. With pre-training, we can achieve an effect of \"understanding\" a language, and the patterns in it, for later downstream task (as demonstrated by the rise of GPT models). We have seen that with enough compute power and electricity, we can practically teach machines to understand *anything*, from photos to videos to audio, and transformers have already made a huge impact on the world, less than a decade after being concieved.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc0fe0f",
   "metadata": {},
   "source": [
    "This notebook's purpose is to try and learn a **linear** decoder of fMRI data (specifically voxel encoded data), that will generalize on unseen fMRI data for completely unrelated concepts. We hope to show that even with a simple linear decoder, without introducing any further complexity to the model, we can achieve true understandinng of a concept, meaning, no matter how it's presented to us (textual, visual data).\n",
    "\n",
    "\n",
    "Before starting the open ended project, we have hypothesized that meaning of textual data (both individual words of concepts and full sentences) and images of the same concepts might be connc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bac71f2",
   "metadata": {},
   "source": [
    "## Text to CLIP\n",
    "This part is the implementation of our hypothesis <br>\n",
    "To try proving said hypothesis, we set out to expand on the work done by Pereira et al. (2018). In their work, _Toward a universal decoder of linguistic meaning from brain activation_, Pereira et al. have made big strides in proving that meaning of different concepts, ranging from abstract to physical objects, is being parsed withing the brain in the same area. They scanned the"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740ac1ec",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2723a1f8",
   "metadata": {},
   "source": [
    "#### Dependecies\n",
    "First, let's download all relevant dependecies to check our hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95771580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ftfy in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (6.3.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: regex in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (2025.7.34)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (3.10.5)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ftfy) (0.2.13)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: gdown in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from gdown) (4.13.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from gdown) (3.18.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from gdown) (2.32.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from beautifulsoup4->gdown) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from beautifulsoup4->gdown) (4.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from requests[socks]->gdown) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from requests[socks]->gdown) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from requests[socks]->gdown) (2025.8.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->gdown) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\user\\appdata\\local\\temp\\pip-req-build-zp70x3u0\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: ftfy in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from clip==1.0) (6.3.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from clip==1.0) (24.2)\n",
      "Requirement already satisfied: regex in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from clip==1.0) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from clip==1.0) (4.67.1)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from clip==1.0) (2.7.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from clip==1.0) (0.22.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (2025.7.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torchvision->clip==1.0) (2.3.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torchvision->clip==1.0) (11.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\user\\AppData\\Local\\Temp\\pip-req-build-zp70x3u0'\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%pip install ftfy regex tqdm scikit-learn numpy matplotlib\n",
    "%pip install -U gdown\n",
    "%pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32aa891",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29101704",
   "metadata": {},
   "source": [
    "##### Extract prerequisite data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ed3675",
   "metadata": {},
   "source": [
    "For this project, we've created a drive folder, containing all of the relevant code and data from the original Pereira et al. paper. Moreover, because the list of concepts and the related images from the original paper is static, we pre-calculated all of the relevant CLIP embeddings (The exact code we used can be seen here in \"one_time_drive_setup\"), and persisted them to drive. The folder contains:\n",
    "- The CLIP embeddings for the textual concepts\n",
    "- The CLIP embedding for all of the photos related to each concept\n",
    "- The list of concepts\n",
    "- The actual images for each concept (zipped)\n",
    "- The fMRI data of one of the participant's in experiment 1 of the Pereira paper <br>\n",
    "\n",
    "All of this data is based on the original Pereira paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8345d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "# If the data already exists, we don't need to download it again\n",
    "if not Path(\"data\").exists():\n",
    "    # Check operating system - handlind the data is done differently on Windows and Linux.\n",
    "    # This will allow us to run the code on Colab, locally, and on any other platform we may choose.\n",
    "    IS_WINDOWS = platform.system() == \"Windows\"\n",
    "    \n",
    "    if IS_WINDOWS:\n",
    "        !python -m gdown --folder --id 1CwmFOsYFnq6t33KAzpvw0gaOTQXbcozs -O ./data/\n",
    "        !powershell -NoProfile -Command \"Expand-Archive -Path ./data/experiment-images.zip -DestinationPath ./data/ -Force\"\n",
    "        !powershell -NoProfile -Command \"Remove-Item ./data/experiment-images.zip\"\n",
    "    else:\n",
    "        !gdown --folder --id 1CwmFOsYFnq6t33KAzpvw0gaOTQXbcozs --output ./data\n",
    "        !unzip ./data/experiment-images.zip\n",
    "        !rm ./data/experiment-images.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0753233",
   "metadata": {},
   "source": [
    "##### Translate extracted data to relevant parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5757d0a",
   "metadata": {},
   "source": [
    "We'll take the extracted data, all in the \"data\" folder of the project, and translate it to relevant parameters. This includes:\n",
    "- Getting the CLIP embeddings for everything\n",
    "- Getting all concepts \n",
    "- Getting fMRI data for textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32b224bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00611386 -0.00396109  0.02534228 ...  0.02644954  0.00413773\n",
      "   0.02350052]\n",
      " [-0.01741413 -0.00905913  0.01194012 ...  0.01327058  0.01946978\n",
      "  -0.00209304]\n",
      " [-0.01952735 -0.00371009  0.03682863 ...  0.03088004  0.00578118\n",
      "   0.03589557]\n",
      " ...\n",
      " [-0.00959127  0.03139894  0.03336199 ... -0.0024904  -0.01428477\n",
      "   0.04504825]\n",
      " [-0.01238144  0.00656633  0.00821481 ...  0.02257253 -0.00517512\n",
      "   0.01862164]\n",
      " [-0.00157958  0.01717282  0.02480505 ...  0.01961724  0.00886472\n",
      "   0.01201813]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "# Define constants\n",
    "TEXT_EMBEDDING_PATH = \"data/clip_text_embeddings.npz\"\n",
    "IMAGE_EMBEDDING_PATH = \"data/clip_image_embeddings.npz\"\n",
    "CONCEPTS_PATH = \"data/concepts.txt\"\n",
    "FMRI_TEXT_PATH = \"data/brain-responses-data/examples_180concepts_wordclouds.mat\"\n",
    "\n",
    "# Get the list of all concepts\n",
    "concepts = np.genfromtxt(CONCEPTS_PATH, dtype=str)\n",
    "# print(concepts)\n",
    "\n",
    "# Get the relevant data for all text embeddings\n",
    "with np.load(TEXT_EMBEDDING_PATH) as text:\n",
    "    text_embeddings = text[\"data\"]\n",
    "print(text_embeddings)\n",
    "\n",
    "# Get the relevant data for all image embeddings\n",
    "with np.load(IMAGE_EMBEDDING_PATH, allow_pickle=True) as imgs:\n",
    "    img_embeddings = np.asarray(imgs[\"embeddings\"], dtype=np.float32)\n",
    "    img_concepts = imgs[\"concepts\"]\n",
    "# print(img_concepts)\n",
    "# print(img_embeddings)\n",
    "\n",
    "# Get the fMRI data, specifically for textual imaging\n",
    "mat = scipy.io.loadmat(FMRI_TEXT_PATH)\n",
    "fmri_text_data = mat[\"examples\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26daed72",
   "metadata": {},
   "source": [
    "##### Symbols Definition\n",
    "- $N$ - Number of rows, being trials or training data\n",
    "- $V$ - Voxel number\n",
    "- $D$ - Dimension of the embedding space (512 for CLIP over the 300 of GloVe)\n",
    "- $C$ - Concepts number (no. of possible candidates, for Pereira will be 180)\n",
    "- $M$ - Raw images number - before aggregating them.\n",
    "- $B$ - Bulk size for leaving out of training for the cross validation process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c52b239",
   "metadata": {},
   "source": [
    "#### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c9fde",
   "metadata": {},
   "source": [
    "##### Normalize concept handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "50133838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['ability', 'accomplished', 'angry', 'apartment', 'applause',\n",
      "       'argument', 'argumentatively', 'art', 'attitude', 'bag', 'ball',\n",
      "       'bar', 'bear', 'beat', 'bed', 'beer', 'big', 'bird', 'blood',\n",
      "       'body', 'brain', 'broken', 'building', 'burn', 'business',\n",
      "       'camera', 'carefully', 'challenge', 'charity', 'charming',\n",
      "       'clothes', 'cockroach', 'code', 'collection', 'computer',\n",
      "       'construction', 'cook', 'counting', 'crazy', 'damage', 'dance',\n",
      "       'dangerous', 'deceive', 'dedication', 'deliberately', 'delivery',\n",
      "       'dessert', 'device', 'dig', 'dinner', 'disease', 'dissolve',\n",
      "       'disturb', 'do', 'doctor', 'dog', 'dressing', 'driver', 'economy',\n",
      "       'election', 'electron', 'elegance', 'emotion', 'emotionally',\n",
      "       'engine', 'event', 'experiment', 'extremely', 'feeling', 'fight',\n",
      "       'fish', 'flow', 'food', 'garbage', 'gold', 'great', 'gun', 'hair',\n",
      "       'help', 'hurting', 'ignorance', 'illness', 'impress', 'invention',\n",
      "       'investigation', 'invisible', 'job', 'jungle', 'kindness', 'king',\n",
      "       'lady', 'land', 'laugh', 'law', 'left', 'level', 'liar', 'light',\n",
      "       'magic', 'marriage', 'material', 'mathematical', 'mechanism',\n",
      "       'medication', 'money', 'mountain', 'movement', 'movie', 'music',\n",
      "       'nation', 'news', 'noise', 'obligation', 'pain', 'personality',\n",
      "       'philosophy', 'picture', 'pig', 'plan', 'plant', 'play',\n",
      "       'pleasure', 'poor', 'prison', 'professional', 'protection',\n",
      "       'quality', 'reaction', 'read', 'relationship', 'religious',\n",
      "       'residence', 'road', 'sad', 'science', 'seafood', 'sell', 'sew',\n",
      "       'sexy', 'shape', 'ship', 'show', 'sign', 'silly', 'sin', 'skin',\n",
      "       'smart', 'smiling', 'solution', 'soul', 'sound', 'spoke', 'star',\n",
      "       'student', 'stupid', 'successful', 'sugar', 'suspect', 'table',\n",
      "       'taste', 'team', 'texture', 'time', 'tool', 'toy', 'tree', 'trial',\n",
      "       'tried', 'typical', 'unaware', 'usable', 'useless', 'vacation',\n",
      "       'war', 'wash', 'weak', 'wear', 'weather', 'willingly', 'word'],\n",
      "      dtype='<U15'), array([  0,   0,   0, ..., 179, 179, 179], shape=(1086,), dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def norm_label(x) -> str:\n",
    "    return str(x).strip().lower()\n",
    "\n",
    "def index_by_concept(concepts: np.ndarray) -> np.ndarray:\n",
    "    normalized = np.array([norm_label(c) for c in concepts], dtype=str)\n",
    "    unique_concepts, concept_ids = np.unique(normalized, return_inverse=True)\n",
    "\n",
    "    return unique_concepts, concept_ids.astype(np.int32)\n",
    "\n",
    "# print(index_by_concept(concepts))\n",
    "print(index_by_concept(img_concepts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc66d09",
   "metadata": {},
   "source": [
    "##### Parse Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6dc612",
   "metadata": {},
   "source": [
    "**row_norm** - This function is getting an `ndarray`, and normalizing each of it rows. We need this kind of normalization because it's necessary for cosine similarity. Similarly, in CLIP, both the image and text embeddings are being L2-normalized to unit length before computing their similarity. Because of that normalization, when the model uses a dot product, it is exactly equal to cosine similarity. We are going to take a similar route in our calculations, and because the embedding need to be normalized both in training and in inference, that justifies creating a small helper function for them in our opinion.<br>\n",
    "The `ndarray` we're getting will be in dimensions $(N, D)$, because each row would be a CLIP embedding vector (and hence a $D$ elements), and there are $N$ rows by definition. <br>\n",
    "For `epsilon`, we'll set it to avoid dividing a vector by zero (the original row of the matrix stays zero, just the norm is capped below by epsilon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "063efc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def row_norm(X: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    L2-normalize each row safely (for cosine). If a row norm is 0, clamp to eps.\n",
    "\n",
    "    Args:\n",
    "        X: (N, D) array\n",
    "        eps: small constant to avoid division by zero\n",
    "\n",
    "    Returns:\n",
    "        (N, D) normalized array\n",
    "    \"\"\"\n",
    "    nrm = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    nrm = np.maximum(nrm, eps)\n",
    "    return X / nrm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c584dfdc",
   "metadata": {},
   "source": [
    "**Textual**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f59534",
   "metadata": {},
   "source": [
    "We saved the embedding of the prompt \"A picture of {c}\", where c is the name of the relevant concept. The reason for that choice is that CLIP reacts very well to prompting, and that embedding is improving results over the non-prompt version. Because we only saved one embedding per concept, the usage of the text embeddings would be very straightfowrward - there is only one way to use them.<br>\n",
    "For further analysis (outside of the scope of this project), because CLIP is so prompt aware, we could consider ensembling of multiple prompts, and handling of multiple text-embedding per concept. <br>\n",
    "Finally, we'll also grab the list of concepts - it would be relevant in later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc8be26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with np.load(\"data/clip_text_embeddings.npz\") as text_embeddings:\n",
    "    clip_text_embeddings = text_embeddings[\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13864109",
   "metadata": {},
   "source": [
    "**Visual**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f67750",
   "metadata": {},
   "source": [
    "The parsing of visual data is much more interesting. In the dataset supplied by the original Pereira paper, there are 6-7 photos for each concept, all of them describing the concept (in contrast to the text which only has one related embedding vectore - the name of the concept). Because we're working with the high-end model of CLIP, embedding takes quite a bit of time. Moreover, the because the input is the same each time, the output embedding vector is also equal for each individual photo. For those reasons, we chose to separate the embedding itself from the main pipeline, and use it as a baseline. The strategy, though, was to embed and save every single picture for every single concept. This was done because when dealing with multiple inputs, it begs the question of what is the best way to process them for later evaluation. <br>\n",
    "Specifically, the possible we checked for processing our base image embeddings are:\n",
    "1. Mean - the \"intuitive\" way to approach multiple inputs per concept, this is just taking all of the embedding vectors, and averaging each of their components to create one \"centralized\" one.\n",
    "2. Medoid - It's a similar way of getting one clustered value from a batch of values, but it differs slightly. Medoid is the data point whose average distance to all other points is smallest.\n",
    "3. Best of K - This is a third way to look at the data, based on the first two. We'll still calculate the average or the medoid for each concept, but instead of clustering ALL of our data for each concept, we'll the the k best ones. For example, a Best of 3 mean approach would mean we'd take the 3 \"best\" image-based embedding vectors of the concept, and take their mean as our final embedding vector. The definition of \"Best\" here would be \"Cosine Similarity to the embedding of the single text-based embedding vector of the same concept\". The choice of cosine similarity to define closeness is extensively discussed further down the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37646f01",
   "metadata": {},
   "source": [
    "Let's define all of the functions we discussed: <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf74a88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "\n",
    "\n",
    "\n",
    "def get_embeddings_mean(embeddings: np.ndarray, concept_ids: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Mean prototype per concept.\n",
    "\n",
    "    Args:\n",
    "        embeddings: (M, D) image embeddings (many rows per concept)\n",
    "        concept_ids: (M,) integer concept id per row\n",
    "\n",
    "    Returns:\n",
    "        concepts: (C_unique,) sorted unique concept ids\n",
    "        protos: (C_unique, D) mean embedding per concept\n",
    "    \"\"\"\n",
    "    concepts = np.unique(concept_ids)\n",
    "    mean_embeddings: List[np.ndarray] = []\n",
    "    for c in concepts:\n",
    "        mean_embeddings.append(embeddings[concept_ids == c].mean(axis=0))\n",
    "    return concepts, np.vstack(mean_embeddings)\n",
    "\n",
    "\n",
    "def get_embeddings_medoid(embeddings: np.ndarray, concept_ids: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Medoid prototype per concept = instance with max mean cosine to others.\n",
    "\n",
    "    Args:\n",
    "        embeddings: (M, D)\n",
    "        concept_ids: (M,)\n",
    "\n",
    "    Returns:\n",
    "        concepts: (C_unique,)\n",
    "        protos: (C_unique, D)\n",
    "    \"\"\"\n",
    "    concepts = np.unique(concept_ids)\n",
    "    protos: List[np.ndarray] = []\n",
    "    for c in concepts:\n",
    "        E = embeddings[concept_ids == c]\n",
    "        En = row_norm(E)\n",
    "        S = En @ En.T  # cosine sims (m x m)\n",
    "        i_medoid = int(np.argmax(S.mean(axis=1)))\n",
    "        protos.append(E[i_medoid])\n",
    "    return concepts, np.vstack(protos)\n",
    "\n",
    "\n",
    "def get_embeddings_topk_by_anchor(\n",
    "    embeddings: np.ndarray,\n",
    "    concept_ids: np.ndarray,\n",
    "    text_by_concept: Dict[int, np.ndarray],\n",
    "    k: int = 3,\n",
    "    agg: str = \"mean\"\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    For each concept, select the top-k image instances by cosine to that concept's\n",
    "    TEXT embedding (anchor), then aggregate by mean or medoid.\n",
    "\n",
    "    Args:\n",
    "        embeddings: (M, D) image embeddings\n",
    "        concept_ids: (M,) integer concept id per row\n",
    "        text_by_concept: dict mapping concept id -> (D,) text embedding\n",
    "        k: number of instances to keep\n",
    "        agg: 'mean' or 'medoid'\n",
    "\n",
    "    Returns:\n",
    "        concepts: (C_unique,)\n",
    "        protos: (C_unique, D)\n",
    "    \"\"\"\n",
    "    concepts = np.unique(concept_ids)\n",
    "    protos: List[np.ndarray] = []\n",
    "    for c in concepts:\n",
    "        E = embeddings[concept_ids == c]\n",
    "        anchor = text_by_concept[int(c)]\n",
    "        En = row_norm(E)\n",
    "        a = anchor / max(np.linalg.norm(anchor), 1e-12)\n",
    "        sims = En @ a\n",
    "        idx = np.argsort(-sims)[:min(k, E.shape[0])]\n",
    "        Ek = E[idx]\n",
    "        if agg == \"mean\" or Ek.shape[0] == 1:\n",
    "            protos.append(Ek.mean(axis=0))\n",
    "        elif agg == \"medoid\":\n",
    "            Ekn = row_norm(Ek)\n",
    "            S = Ekn @ Ekn.T\n",
    "            i_med = int(np.argmax(S.mean(axis=1)))\n",
    "            protos.append(Ek[i_med])\n",
    "        else:\n",
    "            raise ValueError(\"agg must be 'mean' or 'medoid'\")\n",
    "    return concepts, np.vstack(protos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a813228",
   "metadata": {},
   "source": [
    "##### Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2f6eb3",
   "metadata": {},
   "source": [
    "As stated earlier, we are going to keep Pereira et al's method of learning a **linear** decoder, that would be able to generalize from the data it saw to unseen data, and thus capture deeper meaning than the training data itself. In the original paper, they managed to achieve results which are **much** better than chances, implying that a linear decoder is more than sufficient to capture meaning of textual-fMRI data when the data is projected onto an embedding space. <br>\n",
    "We have changed the embedding space from GloVe to CLIP, in order to see if textual-fMRI data can capture meaning of images as well, but we believe that adding extra complexity to the model will defeat our purpose. If our hypothesis is correct, then a linear decoder will be able to capture the meaning of the images in the same embedding space, without adding any non-linearity - a simple model should suffice, as much as is did for the same modality data. <br>\n",
    "For that reason, we are going to train our decoder in the same way that it was trained in the original paper:\n",
    "* For each participant's fMRI data - we're only going to take the top 5000 relevant `voxels`.\n",
    "* For the decoder - we're going to learn a simple ridge regression (using the same code)\n",
    "<br> \n",
    "Another important thing to remember is that we're only going to feed the function **textual** fMRI data, because our hypothesis states that from the same areas in the brain responsible for interpreting textual data can also give us insight on visual data. That means that we will only train our model on textual data, and withold any visual data for evaluation only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89290af8",
   "metadata": {},
   "source": [
    "First, let's define the function that will help us determine the top 5000 relevant voxels: <br>\n",
    "The fMRI data from the experiments consists of a big series of voxel each corresponding to the activation in a different area in the brain. The problem here is that most of the voxels are non-importent and are just noise which will reduce our model's accuracy. For that reason we will clean up the data and only use the 5000 most influencing voxels out of the 200,000 in the original fMRI data and we will be doing so using the select_top_voxels_indexes function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "808cd70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "def mask_fmri_voxels(fmri_data, semantic_vectors, num_voxels=5000):\n",
    "    f_scores = []\n",
    "    for i in tqdm.tqdm(range(semantic_vectors.shape[1])):\n",
    "        f, _ = f_regression(fmri_data, semantic_vectors[:, i])\n",
    "        f_scores.append(f)\n",
    "\n",
    "    f_scores = np.array(f_scores)\n",
    "    voxel_scores = np.max(f_scores, axis=0)\n",
    "    top_voxel_indices = np.argsort(voxel_scores)[-num_voxels:]\n",
    "\n",
    "    return fmri_data[:, top_voxel_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fbdfa0",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f5f9a0",
   "metadata": {},
   "source": [
    "and get only our top voxels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7b9fa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 768/768 [02:39<00:00,  4.80it/s]\n"
     ]
    }
   ],
   "source": [
    "top_voxel_indices = select_top_voxels_indices(fmri_text_data, clip_text_embeddings)\n",
    "reduced_fmri_data = fmri_text_data[:, top_voxel_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815f6c1f",
   "metadata": {},
   "source": [
    "For training the decoder (fitting a linear mapping from brain features, aka. voxels, to our semantic space), we'll use the function \"learn_decoder\", which we took directly from the original paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc27aa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" learn_decoder \"\"\"\n",
    "import sklearn.linear_model\n",
    "\n",
    "def learn_decoder(data, vectors):\n",
    "     \"\"\" Given data (a CxV matrix of V voxel activations per C concepts)\n",
    "     and vectors (a CxD matrix of D semantic dimensions per C concepts)\n",
    "     find a matrix M such that the dot product of M and a V-dimensional \n",
    "     data vector gives a D-dimensional decoded semantic vector. \n",
    "\n",
    "     The matrix M is learned using ridge regression:\n",
    "     https://en.wikipedia.org/wiki/Tikhonov_regularization\n",
    "     \"\"\"\n",
    "     ridge = sklearn.linear_model.RidgeCV(\n",
    "         alphas=[1, 10, .01, 100, .001, 1000, .0001, 10000, .00001, 100000, .000001, 1000000],\n",
    "         fit_intercept=False\n",
    "     )\n",
    "     ridge.fit(data, vectors)\n",
    "     return ridge.coef_.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44e8d25",
   "metadata": {},
   "source": [
    "**fit_decoder** - This function is a small wrapper to the `learn_decoder` function that from the Pereira paper. The point of the wrapper is to:\n",
    "1. Be able to switch out `learn_decoder` for any other fitting function or pipeline - if we wanted to try and fit using elasticnet instead of ridge, or even add non-linearity, for further testing of hypotheses, like trying to optimize for the best voxel mask size, instead of the original 5000 (all of these won't be a part of our analysis, but still, we find that allowing further possible research is an essential part of the research process, as almost all research is built on top of previous research).\n",
    "2. Masking indexes - according to the Pereira paper, if we're masking our training data, we need to mask the same indexes on our test data. This could be done without a function, but saving the masking as a built in part of the process prevents later errors.\n",
    "3. Scaling - We're also scaling our data after training, to standardize the data around the mean 0 and with standard deviation of 1. This helps us to create a robust scale for any application of usage of our embedding features, whether PCA, linear regression, or a full-blown nueral network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49ad541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "@dataclass\n",
    "class TrainedDecoder:\n",
    "    W: np.ndarray                  # (V_sel, D)\n",
    "    scaler: StandardScaler | None  # fitted on train only\n",
    "    mask: np.ndarray | None        # boolean or int indices for selected voxels\n",
    "\n",
    "def fit_decoder(train_X, train_Y, learn_decoder_fn, use_scaler=True, mask=None):\n",
    "    \"\"\"\n",
    "    train_X: (N_train, V) brain features\n",
    "    train_Y: (N_train, D) semantic targets (text space)\n",
    "    mask:    optional voxel selection (fit on train only)\n",
    "    Returns: TrainedDecoder(W, scaler, mask)\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler().fit(train_X) if use_scaler else None\n",
    "    Xs = scaler.transform(train_X) if scaler else train_X\n",
    "    Xs = Xs[:, mask] if mask is not None else Xs\n",
    "\n",
    "    # This is exactly your ridge-based learner:\n",
    "    W = learn_decoder_fn(Xs, train_Y)  # shape (V_sel, D)\n",
    "    return TrainedDecoder(W=W, scaler=scaler, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f44bc1b",
   "metadata": {},
   "source": [
    "We're now ready to train our decoder, but we would like to do so not on all of the avaliable data - it is very scarce, and so we want to run k-fold cross validation on the decoder, and actually train and evaluate a bunch of decoders as part of the same pipeline, and plot out the result as an average, to get a better sense of the true preformance of our decoder, and thus, our hypothesis. Theoretically, the code to train a decoder on all of the available textual data would be as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b7d1741",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = learn_decoder(reduced_fmri_data, clip_text_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f033483",
   "metadata": {},
   "source": [
    "But we're going to train the actual decoders we'll use as a part of the evaluation pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bd92eb",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6751b10",
   "metadata": {},
   "source": [
    "**decoder** - This function is for our inference, and is simply doing the following things:\n",
    "1. Apply the same scaler and mask we've applied in the `fit_decoder` function.\n",
    "2. Doing a dot product on the scaled decoder (which contains all our \"learned parameters\", per se).\n",
    "\n",
    "Again, we're covering our bases here with a generic wrapper, as we want our code to possibly be used in many future research directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a586464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(model: TrainedDecoder, X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply the same scaler and voxel mask, then multiply by W.\n",
    "\n",
    "    Args:\n",
    "        model: TrainedDecoder returned by fit_decoder\n",
    "        X: (N, V) brain features to decode\n",
    "\n",
    "    Returns:\n",
    "        (N, D) decoded semantic vectors\n",
    "    \"\"\"\n",
    "    Xs = model.scaler.transform(X) if model.scaler is not None else X\n",
    "    if model.mask is not None:\n",
    "        Xs = Xs[:, model.mask]\n",
    "    return Xs @ model.W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2901909",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0c9612",
   "metadata": {},
   "source": [
    "For evaluation, we're going to have the following guiding principles:\n",
    "1. Distance function (cosine). We measure similarity with cosine. In distributional semantics, cosine best reflects semantic relatedness and was explicitly used in Pereira’s pipeline, so our evaluation geometry matches the embedding space and prior work.\n",
    "2. K-Fold Cross Validation - Quality fMRI data is extremely limited. Pereira is still the only open English dataset for fMRI cross-modality, single concept data, similar to what we're trying to model. There are some other alternatives - Allen 672 which is Chinese, Tuckute 2024, which is for full sentences only, six words each, each of them on no more than 16 participants. That means we were little available data to train on. Because of that fact, we'll choose to evaluate the data using K-Fold Cross Validation - that way, we can train on every piece of data we have, and still evaluate the model. The folding strategy would be as follows - we're going to train a set of 18 decoders, each trained on 170 concepts, and evaluated on the remaining 10 only. The evaluation would be split into two parts:\n",
    "* Textual trials (to confirm within-modality performance)\n",
    "* Visual trials (to test the hypothesis that text-trained decoders generalize cross-modally)\n",
    "For the eval, we'll evaluate on the concepts we didn't train on - both on the visual and the textual data. This is to prevent data leakage - nothing about a test concept appears in training.\n",
    "3. Primary metric: rank accuracy (normalized). For each held-out stimulus, we feed its brain image into the decoder to produce a semantic vector (the decoder maps brain→text semantic space), then compare it by cosine to all candidate vectors in the same evaluation set and compute rank accuracy (normalized):\n",
    "$$\n",
    "\\text{rank\\_accuracy} \\;=\\; 1 \\;-\\; \\frac{\\text{rank} - 1}{\\#\\text{candidates} - 1}\n",
    "$$\n",
    "This yields a score in [0, 1] with chance = 0.5, and 1.0 being a perfect decoder (always spits out the correct answer); it’s exactly the statistic used in Pereira."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cff187",
   "metadata": {},
   "source": [
    "##### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb3b4cd",
   "metadata": {},
   "source": [
    "**cosine_rank** - This function is the core of our main metric of evaluation on a single test trial. We get a `decoded vector`, already a part of the CLIP embedding space (dimension $D$), and we get it's `true index` in the concept list (just an int, indicating it's place in the sorted array of the concepts by name), and all the embeddings of the `candidate concepts`, compiled to a one big matrix, with the dimesions $(C,D)$ where $C$ is the number of concepts, and $D$ is the dimension of our embedding space. This is a 1-based rank, meaning, we're not going for accuracy of the decoder yet, just how close we are for this specific test case to the true concept. Our calculation will work as follows:\n",
    "1. Normalize everything (both the decoded vector, and the matrix of all the embedded concepts, so everything that's embedded in our embedding space)\n",
    "2. Calculate cosine similarity using a dot product (Cosine similarity between two vectors a and b is just the dot product of their unit versions.)\n",
    "3. Sort all of the cosine similarities in a descending order, to find out where the `true_idx` sits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbc4a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_rank(decoded_vec: np.ndarray, true_idx: int, candidate_matrix: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    1-based rank of the true candidate by cosine similarity.\n",
    "\n",
    "    Args:\n",
    "        decoded_vec: (D,) decoded semantic vector for a single test trial\n",
    "        true_idx: integer index of the correct row in candidate_matrix\n",
    "        candidate_matrix: (C, D) candidate concept embeddings\n",
    "\n",
    "    Returns:\n",
    "        1-based rank in [1..C] (1 = best)\n",
    "    \"\"\"\n",
    "    dv = decoded_vec / max(np.linalg.norm(decoded_vec), 1e-12)\n",
    "    Cn = row_norm(candidate_matrix)\n",
    "    sims = Cn @ dv\n",
    "    order = np.argsort(-sims)  # descending\n",
    "    return int(np.where(order == true_idx)[0][0] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b953f",
   "metadata": {},
   "source": [
    "**rank_accuracy** - This function is for actually evaluating a decoder. As part of the evaluation startegy we've outlined, we said that we're going to do a cross validation, training $C / B$ decoders, each with $C - B$ training samples, and $B$ test samples. For evaluating the decoder, we outlined our main metric as rank_accuracy, which is: _across all test samples, how accurate is a decoder (1.0 being always correct, 0.5 chance value)_. This is exactly what this function does - we're getting all of our decoded test vectors, a matrix of dimensions $(N,D)$, which is $N$ test vectors of $D$ embedding elements each, an array of $N$ test true indexes for the concepts, and the entire concept embeddings, which, as earlier, is $(C,D)$. We then:\n",
    "1. Calculate the ranks of all of our test decoded vectors.\n",
    "2. Calculate the accuracy of the decoder, the mean rank, and any other matric we want for that decoder (this function works for a single decoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3e06d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "def rank_accuracy(\n",
    "    decoded: np.ndarray,\n",
    "    true_indices: np.ndarray,\n",
    "    candidates: np.ndarray\n",
    ") -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    Compute raw ranks per test row, mean raw rank, and normalized rank accuracy.\n",
    "\n",
    "    Args:\n",
    "        decoded: (N_test, D) decoded vectors for the test rows\n",
    "        true_indices: (N_test,) integer index per row into 'candidates'\n",
    "        candidates: (C, D) candidate concept embeddings\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"ranks\": np.ndarray of shape (N_test,), dtype=int (1..C),\n",
    "          \"mean_rank\": float,\n",
    "          \"rank_accuracy\": float in [0,1], chance = 0.5\n",
    "        }\n",
    "    \"\"\"\n",
    "    ranks = np.array(\n",
    "        [cosine_rank(decoded[i], int(true_indices[i]), candidates) for i in range(decoded.shape[0])],\n",
    "        dtype=int\n",
    "    )\n",
    "    C = candidates.shape[0]\n",
    "    mean_rank = float(ranks.mean()) if ranks.size else float('nan')\n",
    "    # Normalized rank accuracy (Pereira): 1 - (mean_rank - 1)/(C - 1)\n",
    "    rank_acc = 1.0 - (mean_rank - 1.0) / (C - 1.0) if C > 1 and np.isfinite(mean_rank) else float('nan')\n",
    "    return {\"ranks\": ranks, \"mean_rank\": mean_rank, \"rank_accuracy\": rank_acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6843bf09",
   "metadata": {},
   "source": [
    "The beauty of all of our helper functions is that they are modal agnostic. Meaning, they're just getting CLIP embeddings, and do not care whether the data originated in textual or visual inputs, in fMRI, or in anything else. This means that the implementation is extremely generic, and we can switch practically everything we want around - as long as the main way to calculate \"distance\" between two vectors is cosine similarity, and not Euclidean distance, for example. Because the approach of cosine similiarity is very popular, and the most prevalent in practically all embedding spaces, this gives us (and potentially future researches) a ton of freedom to work with - The pipeline will be able to work on practically any input data, as long as it's possible to project it onto a relevant embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e435fe2",
   "metadata": {},
   "source": [
    "##### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0af032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_folds_by_concept(concept_ids: np.ndarray, fold_size: int) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Create concept-grouped folds: each test fold holds out 'fold_size' concepts.\n",
    "\n",
    "    Args:\n",
    "        concept_ids: (N,) concept label per row of the *training* matrix you'll split\n",
    "        fold_size: number of concepts in each test fold (e.g., 10)\n",
    "\n",
    "    Returns:\n",
    "        List of (train_idx, test_idx) index arrays into the rows of the input arrays\n",
    "    \"\"\"\n",
    "    uniq = np.unique(concept_ids)\n",
    "    folds: List[Tuple[np.ndarray, np.ndarray]] = []\n",
    "    for start in range(0, len(uniq), fold_size):\n",
    "        held = uniq[start:start + fold_size]\n",
    "        test_mask = np.isin(concept_ids, held)\n",
    "        train_idx = np.where(~test_mask)[0]\n",
    "        test_idx = np.where(test_mask)[0]\n",
    "        folds.append((train_idx, test_idx))\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74267d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_validate_once_per_fold(\n",
    "    X_brain: np.ndarray,             # (N, V) rows aligned with Y_targets & concept_ids\n",
    "    Y_targets: np.ndarray,           # (N, D)\n",
    "    concept_ids: np.ndarray,         # (N,)\n",
    "    fold_size: int,\n",
    "    learn_decoder_fn,                # your learn_decoder\n",
    "    eval_sets: Dict[str, Dict[str, np.ndarray]],\n",
    "    use_scaler: bool = True,\n",
    "    make_mask_fn = None             # optional callable(train_X, train_Y) -> mask (per fold)\n",
    ") -> Dict[str, List[Dict[str, object]]]:\n",
    "    \"\"\"\n",
    "    Train the decoder once per fold on the training split, then evaluate it on\n",
    "    multiple evaluation sets for *held-out concepts only*.\n",
    "\n",
    "    Args:\n",
    "        X_brain: (N, V) brain data used to train (e.g., text trials only)\n",
    "        Y_targets: (N, D) target text-space embeddings aligned to X_brain\n",
    "        concept_ids: (N,) concept id per row in X_brain (used to define folds)\n",
    "        fold_size: number of concepts in each test fold (e.g., 10)\n",
    "        learn_decoder_fn: your learn_decoder\n",
    "        eval_sets: dict mapping eval-name -> dict with:\n",
    "            {\n",
    "              \"X\": (N_eval, V),                   # brain rows to decode\n",
    "              \"concept_ids\": (N_eval,),           # concept id per row\n",
    "              \"candidates\": (C, D),               # candidate matrix to rank against\n",
    "              \"candidate_concepts\": (C,)          # concept ids in same order as rows in 'candidates'\n",
    "            }\n",
    "        use_scaler: fit StandardScaler on training split\n",
    "        make_mask_fn: optional per-fold voxel selection function fit on training split\n",
    "\n",
    "    Returns:\n",
    "        results: dict eval-name -> list (per fold) of metrics dicts from ranks_and_accuracy\n",
    "    \"\"\"\n",
    "    # Prepare result containers\n",
    "    results: Dict[str, List[Dict[str, object]]] = {name: [] for name in eval_sets.keys()}\n",
    "\n",
    "    # Build concept-grouped folds on the training set's concept ids\n",
    "    folds = make_folds_by_concept(concept_ids, fold_size)\n",
    "\n",
    "    # For each fold: train once, evaluate across all eval sets\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(folds):\n",
    "        # Optional voxel selection (fit on training split only)\n",
    "        mask = None\n",
    "        if make_mask_fn is not None:\n",
    "            mask = make_mask_fn(X_brain[train_idx], Y_targets[train_idx])\n",
    "\n",
    "        # Fit decoder\n",
    "        model = fit_decoder(\n",
    "            train_X=X_brain[train_idx],\n",
    "            train_Y=Y_targets[train_idx],\n",
    "            learn_decoder_fn=learn_decoder_fn,\n",
    "            use_scaler=use_scaler,\n",
    "            mask=mask\n",
    "        )\n",
    "\n",
    "        # The set of held-out concepts for this fold (used to filter eval sets)\n",
    "        held_concepts = np.unique(concept_ids[test_idx])\n",
    "\n",
    "        # Evaluate against each eval view\n",
    "        for name, spec in eval_sets.items():\n",
    "            X_eval_all   = spec[\"X\"]\n",
    "            cid_eval_all = spec[\"concept_ids\"]\n",
    "            Cmat         = spec[\"candidates\"]\n",
    "            Cconcepts    = spec[\"candidate_concepts\"]\n",
    "\n",
    "            # Select only rows whose concept is in the held-out set\n",
    "            mask_rows = np.isin(cid_eval_all, held_concepts)\n",
    "            if not np.any(mask_rows):\n",
    "                # Nothing to evaluate for this view in this fold\n",
    "                results[name].append({\"ranks\": np.array([], dtype=int), \"mean_rank\": float('nan'), \"rank_accuracy\": float('nan')})\n",
    "                continue\n",
    "\n",
    "            X_eval = X_eval_all[mask_rows]\n",
    "            cid_eval = cid_eval_all[mask_rows]\n",
    "\n",
    "            # Map each eval row's concept id to the index in the candidate matrix\n",
    "            concept_to_idx = {int(c): i for i, c in enumerate(Cconcepts)}\n",
    "            try:\n",
    "                true_indices = np.array([concept_to_idx[int(c)] for c in cid_eval], dtype=int)\n",
    "            except KeyError as e:\n",
    "                missing = int(e.args[0])\n",
    "                raise KeyError(f\"Concept id {missing} not found in candidate_concepts for eval set '{name}'. \"\n",
    "                               \"Ensure candidate_concepts matches your concept_id space.\") from e\n",
    "\n",
    "            # Decode brain -> semantic\n",
    "            decoded = decode(model, X_eval)\n",
    "\n",
    "            # Rank metrics\n",
    "            metrics = rank_accuracy(decoded, true_indices, Cmat)\n",
    "            results[name].append(metrics)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0488a9a0",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "979ddec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_cv_results(results: Dict[str, List[Dict[str, object]]]) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Compute simple per-view averages across folds.\n",
    "\n",
    "    Args:\n",
    "        results: dict eval-name -> list of per-fold metrics\n",
    "\n",
    "    Returns:\n",
    "        summary: dict eval-name -> {\"rank_accuracy\": float, \"mean_rank\": float}\n",
    "    \"\"\"\n",
    "    summary: Dict[str, Dict[str, float]] = {}\n",
    "    for name, folds in results.items():\n",
    "        ra = np.nanmean([f.get(\"rank_accuracy\", np.nan) for f in folds])\n",
    "        mr = np.nanmean([f.get(\"mean_rank\", np.nan) for f in folds])\n",
    "        summary[name] = {\"rank_accuracy\": float(ra), \"mean_rank\": float(mr)}\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ecda67",
   "metadata": {},
   "source": [
    "#### Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ae6056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def DecVTC_pipeline(\n",
    "    text_embeddings: np.ndarray,\n",
    "    img_embeddings: np.ndarray,\n",
    "    img_concepts: np.ndarray,\n",
    "    concepts: np.ndarray,\n",
    "    fmri_data: np.ndarray,\n",
    "    grouping_method: str, # mean/medoid/topk|k|agg\n",
    "    # evaluation_sets: dict,\n",
    "    learn_decoder_fn,\n",
    "    mask_fn,\n",
    "    mask_size: int = 5000,\n",
    "    fold_size: int = 10\n",
    "):\n",
    "    # ---------- Pre-process ----------\n",
    "    # Normalize embeddings\n",
    "    text_embeddings = row_norm(text_embeddings)\n",
    "    img_embeddings = row_norm(img_embeddings)\n",
    "    \n",
    "    # Index concepts\n",
    "    unique_concepts, concept_ids = index_by_concept(concepts)\n",
    "    unique_img_concepts, img_concept_ids = index_by_concept(img_concepts)\n",
    "    assert np.array_equal(unique_concepts, unique_img_concepts), \"Image and text concepts do not match.\"\n",
    "\n",
    "   # Group embeddings\n",
    "    if grouping_method == \"mean\":\n",
    "        candidate_concepts, grouped_img_embeddings = get_embeddings_mean(img_embeddings, img_concept_ids)\n",
    "    elif grouping_method == \"medoid\":\n",
    "        candidate_concepts, grouped_img_embeddings = get_embeddings_medoid(img_embeddings, img_concept_ids)\n",
    "    elif \"topk\" in grouping_method:\n",
    "        k   = int(grouping_method.split('|')[1])\n",
    "        agg = grouping_method.split('|')[2]\n",
    "\n",
    "        # anchor = text embedding per concept id (mean over that concept’s rows)\n",
    "        concept_anchor = {\n",
    "            c: text_embeddings[concept_ids == c].mean(axis=0)\n",
    "            for c in np.unique(concept_ids)\n",
    "        }\n",
    "\n",
    "        candidate_concepts, grouped_img_embeddings = get_embeddings_topk_by_anchor(\n",
    "            embeddings=img_embeddings,\n",
    "            concept_ids=img_concept_ids,\n",
    "            text_by_concept=concept_anchor,\n",
    "            k=k,\n",
    "            agg=agg\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown grouping method: {grouping_method}\")\n",
    "\n",
    "\n",
    "    # Mask fmri data\n",
    "    masked_fmri = mask_fn(fmri_data, text_embeddings, mask_size)\n",
    "\n",
    "    features      = np.asarray(masked_fmri, dtype=np.float32)         # (N, V)\n",
    "    text_targets  = np.asarray(text_embeddings, dtype=np.float32)      # (N, D)\n",
    "    image_targets = np.asarray(grouped_img_embeddings, dtype=np.float32)  # (C, D)\n",
    "\n",
    "    eval_sets = {\n",
    "        \"text\": {\n",
    "            \"X\": features,\n",
    "            \"concept_ids\": concept_ids,               # (N,)\n",
    "            \"candidates\": text_targets,               # (N, D)\n",
    "            \"candidate_concepts\": concept_ids         # (N,)\n",
    "        },\n",
    "        \"image\": {\n",
    "            \"X\": features,\n",
    "            \"concept_ids\": concept_ids,               # (N,)\n",
    "            \"candidates\": image_targets,              # (C, D)\n",
    "            \"candidate_concepts\": candidate_concepts  # (C,)\n",
    "        }\n",
    "}\n",
    "\n",
    "    # ---------- Cross Validate ----------\n",
    "    results = cross_validate_once_per_fold(\n",
    "        X_brain=features,\n",
    "        Y_targets=text_targets,\n",
    "        concept_ids=concept_ids,\n",
    "        fold_size=fold_size,\n",
    "        learn_decoder_fn=learn_decoder_fn,\n",
    "        eval_sets=eval_sets,\n",
    "        use_scaler=True,\n",
    "        make_mask_fn=None\n",
    "    )\n",
    "\n",
    "    # ---------- Analyze and output ----------\n",
    "    summary = summarize_cv_results(results)\n",
    "    return results, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca62a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/768 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 768/768 [02:03<00:00,  6.23it/s]\n"
     ]
    }
   ],
   "source": [
    "results, summary = DecVTC_pipeline(\n",
    "    text_embeddings=text_embeddings,\n",
    "    img_embeddings=img_embeddings,\n",
    "    img_concepts=img_concepts,\n",
    "    concepts=concepts,\n",
    "    fmri_data=fmri_text_data,\n",
    "    grouping_method=\"topk|3|mean\",\n",
    "    learn_decoder_fn=learn_decoder,\n",
    "    mask_fn=mask_fmri_voxels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef88d8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': [{'ranks': array([38, 87, 82,  6, 21,  9,  7,  7, 27, 90]), 'mean_rank': 37.4, 'rank_accuracy': 0.7966480446927374}, {'ranks': array([ 20,  23,  19,  86,  14, 151,  47,  10,  52,  71]), 'mean_rank': 49.3, 'rank_accuracy': 0.7301675977653632}, {'ranks': array([88, 84,  7, 36, 24, 80, 37, 61, 85, 31]), 'mean_rank': 53.3, 'rank_accuracy': 0.7078212290502793}, {'ranks': array([17, 23, 67, 90, 80, 91,  5, 43, 65, 88]), 'mean_rank': 56.9, 'rank_accuracy': 0.6877094972067039}, {'ranks': array([ 89,  11, 147,   8,  10,  99,   9,   6,  81,  45]), 'mean_rank': 50.5, 'rank_accuracy': 0.723463687150838}, {'ranks': array([31, 14,  4, 77, 53,  2, 71, 31, 85, 23]), 'mean_rank': 39.1, 'rank_accuracy': 0.7871508379888268}, {'ranks': array([ 14,  10,  14,   5,  12,  58,  40,  48, 159,  78]), 'mean_rank': 43.8, 'rank_accuracy': 0.7608938547486034}, {'ranks': array([141, 114,   4,  23,  12,  30,  36, 103,  41,   2]), 'mean_rank': 50.6, 'rank_accuracy': 0.722905027932961}, {'ranks': array([ 49, 101,  48, 116,  16,  13,  10,  28, 107,  31]), 'mean_rank': 51.9, 'rank_accuracy': 0.7156424581005587}, {'ranks': array([ 27,  94,  79,  66,  88,  90,  57,  48, 144, 115]), 'mean_rank': 80.8, 'rank_accuracy': 0.5541899441340783}, {'ranks': array([30, 50, 24, 56, 43, 16, 85, 81, 26, 46]), 'mean_rank': 45.7, 'rank_accuracy': 0.7502793296089385}, {'ranks': array([ 95,  84,   2,   4,  74,  69,  35,  27, 149,  28]), 'mean_rank': 56.7, 'rank_accuracy': 0.6888268156424581}, {'ranks': array([ 95,   7,  51,  40,  73,  51, 131,  39,  70,  85]), 'mean_rank': 64.2, 'rank_accuracy': 0.6469273743016759}, {'ranks': array([ 79,   9,  18,   7,  50, 156,  90,  36,  36,  56]), 'mean_rank': 53.7, 'rank_accuracy': 0.705586592178771}, {'ranks': array([ 20,  69, 151,  42,   4,  18,  88,  85,  28, 121]), 'mean_rank': 62.6, 'rank_accuracy': 0.6558659217877094}, {'ranks': array([107,  50,  71,  48,  25,  51,  62,  57,  69,  37]), 'mean_rank': 57.7, 'rank_accuracy': 0.6832402234636872}, {'ranks': array([ 87,  84, 108,  30,  26,  59, 163,  55,  34, 135]), 'mean_rank': 78.1, 'rank_accuracy': 0.5692737430167598}, {'ranks': array([ 43,  78,  71,  57,  72,  34,  79,  57,  44, 103]), 'mean_rank': 63.8, 'rank_accuracy': 0.6491620111731844}], 'image': [{'ranks': array([121,  76,  47,   6,  71,   2,  26,  11, 101, 149]), 'mean_rank': 61.0, 'rank_accuracy': 0.664804469273743}, {'ranks': array([ 53,  69, 109,   7,  32,   6, 147,  23,  21, 101]), 'mean_rank': 56.8, 'rank_accuracy': 0.688268156424581}, {'ranks': array([115, 107,  20, 106,  30,  67,  77,  71,  24,  51]), 'mean_rank': 66.8, 'rank_accuracy': 0.6324022346368715}, {'ranks': array([162,  97, 114,  53,  84,  54, 112,  98, 126, 120]), 'mean_rank': 102.0, 'rank_accuracy': 0.43575418994413406}, {'ranks': array([ 72, 104, 139,   7, 139, 137,   1,  84, 133,  18]), 'mean_rank': 83.4, 'rank_accuracy': 0.5396648044692738}, {'ranks': array([ 93, 133,   8,  71,  80,  29,  41,  55,  47,  50]), 'mean_rank': 60.7, 'rank_accuracy': 0.6664804469273743}, {'ranks': array([ 10,  57,  27,  91,  53,  85,  21, 102,  38,  30]), 'mean_rank': 51.4, 'rank_accuracy': 0.7184357541899442}, {'ranks': array([118,  82,  15,  14,   3,  78, 100, 113,  22,   7]), 'mean_rank': 55.2, 'rank_accuracy': 0.6972067039106145}, {'ranks': array([ 21,  30,  47, 155,   8,  76,  78,  24, 106,  35]), 'mean_rank': 58.0, 'rank_accuracy': 0.6815642458100559}, {'ranks': array([10, 86, 47, 23, 46, 39, 17, 83, 18, 99]), 'mean_rank': 46.8, 'rank_accuracy': 0.7441340782122905}, {'ranks': array([57, 74, 99, 86, 58, 14, 97, 20, 49, 40]), 'mean_rank': 59.4, 'rank_accuracy': 0.6737430167597765}, {'ranks': array([ 46,  78,  15,  91, 169, 166,   2,  38, 100,  16]), 'mean_rank': 72.1, 'rank_accuracy': 0.6027932960893856}, {'ranks': array([149,  11,  83,  20,  25, 114,  93,  54,  97, 106]), 'mean_rank': 75.2, 'rank_accuracy': 0.5854748603351956}, {'ranks': array([ 25,   3,  17,   4, 179, 178,  70, 151,  27,  39]), 'mean_rank': 69.3, 'rank_accuracy': 0.6184357541899441}, {'ranks': array([  3,  22, 171,  10, 151,  65,  95,  50,  63,  59]), 'mean_rank': 68.9, 'rank_accuracy': 0.6206703910614525}, {'ranks': array([ 50,   6,  16, 115,   7,  22, 148,  21, 160, 138]), 'mean_rank': 68.3, 'rank_accuracy': 0.6240223463687151}, {'ranks': array([132,  32, 126, 122, 129, 124, 180,  89,  60, 165]), 'mean_rank': 115.9, 'rank_accuracy': 0.35810055865921786}, {'ranks': array([ 32,  54,  80,  72, 152,  69,  38,  74,  24,  49]), 'mean_rank': 64.4, 'rank_accuracy': 0.6458100558659218}]}\n",
      "{'text': {'rank_accuracy': 0.6964307883302296, 'mean_rank': 55.33888888888889}, 'image': {'rank_accuracy': 0.6220980757293607, 'mean_rank': 68.64444444444446}}\n"
     ]
    }
   ],
   "source": [
    "print(results)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1987ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- label normalization (prevents “cat ” vs “Cat” mismatches) ---\n",
    "def _norm_label(x) -> str:\n",
    "    return str(x).strip().lower()\n",
    "\n",
    "def run_cv_text_and_images_using_your_prototypes(\n",
    "    image_npz_path: str,\n",
    "    reduced_fmri_data: np.ndarray,     # (C, V)\n",
    "    clip_text_embeddings: np.ndarray,  # (C, D)\n",
    "    concept_labels: np.ndarray,        # (C,) strings, aligned to rows of fmri/text\n",
    "    *,\n",
    "    fold_size: int = 10,\n",
    "    image_proto: str = \"topk\",         # \"mean\" | \"medoid\" | \"topk\"\n",
    "    k: int = 3,\n",
    "    agg: str = \"mean\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Uses YOUR prototypes_mean / prototypes_medoid / prototypes_topk_by_text_anchor,\n",
    "    and your cross_validate_once_per_fold (which expects rank_accuracy).\n",
    "    Assumes you have: learn_decoder, fit_decoder, decode, cross_validate_once_per_fold, rank_accuracy\n",
    "    \"\"\"\n",
    "    # 1) Load image embeddings\n",
    "    pack = np.load(image_npz_path, allow_pickle=True)\n",
    "    img_embeddings = np.asarray(pack[\"embeddings\"], dtype=np.float32)  # (M, D)\n",
    "    img_concepts_raw = pack[\"concepts\"]                                 # (M,) strings\n",
    "\n",
    "    # 2) Normalize labels for robust mapping\n",
    "    concept_labels = np.asarray(concept_labels)\n",
    "    labels_norm = np.array([_norm_label(s) for s in concept_labels])\n",
    "    label_to_id = {lab: i for i, lab in enumerate(labels_norm)}\n",
    "\n",
    "    img_concepts_norm = np.array([_norm_label(s) for s in img_concepts_raw])\n",
    "    unknown = np.setdiff1d(np.unique(img_concepts_norm), labels_norm)\n",
    "    if unknown.size > 0:\n",
    "        # Show a few to help you fix concepts.txt vs folder names\n",
    "        raise ValueError(\n",
    "            f\"{unknown.size} image concepts not found in concept_labels. \"\n",
    "            f\"First few: {unknown[:10].tolist()}.\\n\"\n",
    "            \"Make sure 'concepts.txt' corresponds to your image folder names (case/space-insensitive).\"\n",
    "        )\n",
    "\n",
    "    img_cids = np.array([label_to_id[s] for s in img_concepts_norm], dtype=int)\n",
    "\n",
    "    # 3) Build image prototypes using YOUR functions\n",
    "    #    (these must be imported/defined already)\n",
    "    if image_proto == \"mean\":\n",
    "        uniq_img_concepts, C_img = prototypes_mean(img_embeddings, img_cids)\n",
    "    elif image_proto == \"medoid\":\n",
    "        uniq_img_concepts, C_img = prototypes_medoid(img_embeddings, img_cids)\n",
    "    elif image_proto == \"topk\":\n",
    "        # Build dict[int -> (D,)] expected by your prototypes_topk_by_text_anchor\n",
    "        text_by_concept = {i: clip_text_embeddings[i] for i in range(clip_text_embeddings.shape[0])}\n",
    "        uniq_img_concepts, C_img = prototypes_topk_by_text_anchor(\n",
    "            embeddings=img_embeddings,\n",
    "            concept_ids=img_cids,\n",
    "            text_by_concept=text_by_concept,\n",
    "            k=k,\n",
    "            agg=agg\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"image_proto must be 'mean', 'medoid', or 'topk'\")\n",
    "\n",
    "    # 4) Pack eval sets: text and image\n",
    "    X = np.asarray(reduced_fmri_data, dtype=np.float32)\n",
    "    Y = np.asarray(clip_text_embeddings, dtype=np.float32)\n",
    "    C = X.shape[0]\n",
    "    assert Y.shape[0] == C, f\"FMRI and text embeddings row mismatch: {X.shape} vs {Y.shape}\"\n",
    "\n",
    "    concept_ids = np.arange(C, dtype=int)\n",
    "\n",
    "    eval_sets = {\n",
    "        \"text\": {\n",
    "            \"X\": X,\n",
    "            \"concept_ids\": concept_ids,\n",
    "            \"candidates\": Y,                      # text candidates\n",
    "            \"candidate_concepts\": concept_ids\n",
    "        },\n",
    "        \"image\": {\n",
    "            \"X\": X,\n",
    "            \"concept_ids\": concept_ids,\n",
    "            \"candidates\": C_img,                  # image prototypes (subset of concepts)\n",
    "            \"candidate_concepts\": uniq_img_concepts\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # 5) Run your CV driver (train once per fold on text → evaluate both views)\n",
    "    results = cross_validate_once_per_fold(\n",
    "        X_brain=X,\n",
    "        Y_targets=Y,\n",
    "        concept_ids=concept_ids,\n",
    "        fold_size=fold_size,\n",
    "        learn_decoder_fn=learn_decoder,\n",
    "        eval_sets=eval_sets,\n",
    "        use_scaler=True,\n",
    "        make_mask_fn=None\n",
    "    )\n",
    "\n",
    "    # 6) Summarize\n",
    "    def _summarize(res):\n",
    "        out = {}\n",
    "        for name, folds in res.items():\n",
    "            ra = np.nanmean([f[\"rank_accuracy\"] for f in folds])\n",
    "            mr = np.nanmean([f[\"mean_rank\"] for f in folds])\n",
    "            out[name] = {\"rank_accuracy\": float(ra), \"mean_rank\": float(mr)}\n",
    "        return out\n",
    "\n",
    "    summary = _summarize(results)\n",
    "    print(\"CV summary:\")\n",
    "    for view, s in summary.items():\n",
    "        print(f\" - {view:>5s}: rank_acc={s['rank_accuracy']:.3f}  mean_rank={s['mean_rank']:.1f}  (chance rank_acc=0.5)\")\n",
    "    return results, summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d67f3270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV summary:\n",
      " -  text: rank_acc=0.696  mean_rank=55.3  (chance rank_acc=0.5)\n",
      " - image: rank_acc=0.633  mean_rank=66.6  (chance rank_acc=0.5)\n"
     ]
    }
   ],
   "source": [
    "concepts = np.genfromtxt(\"data/concepts.txt\", dtype=str)\n",
    "\n",
    "results, summary = run_cv_text_and_images_using_your_prototypes(\n",
    "    image_npz_path=\"data/clip_image_embeddings.npz\",\n",
    "    reduced_fmri_data=reduced_fmri_data,\n",
    "    clip_text_embeddings=clip_text_embeddings,\n",
    "    concept_labels=np.array(concepts),\n",
    "    fold_size=8,\n",
    "    image_proto=\"topk\",   # or \"mean\", \"medoid\"\n",
    "    k=3,\n",
    "    agg=\"mean\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0c7b91",
   "metadata": {},
   "source": [
    "## Multimodal to CLIP\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
