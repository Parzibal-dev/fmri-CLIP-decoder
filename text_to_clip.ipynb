{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bac71f2",
   "metadata": {},
   "source": [
    "### Text to CLIP cross modality ###\n",
    "This notebook is the implementation of our original hypothesis <br>\n",
    "Before starting the open ended project, we have hypothesized that meaning of textual data (both individual words of concepts and full sentences) and images of the same concepts might be interpreted in the same places inside the brain. <br>\n",
    "To try proving said hypothesis, we set out to expand on the work done by Pereira et al. (2018). In their work, \"Toward a universal decoder of linguistic meaning from brain activation\", Pereira et al. have made big strides in proving that meaning of different concepts, ranging from abstract to physical objects, is being parsed withing the brain in the same area. They scanned the"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740ac1ec",
   "metadata": {},
   "source": [
    "#### Setup ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2723a1f8",
   "metadata": {},
   "source": [
    "##### Dependecies #####\n",
    "First, let's download all relevant dependecies to check our hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95771580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting regex\n",
      "  Downloading regex-2025.7.34-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.0.1)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.5-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ftfy) (0.2.13)\n",
      "Requirement already satisfied: colorama in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn) (3.5.0)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.59.0-cp312-cp312-win_amd64.whl.metadata (110 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.3.0-cp312-cp312-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Downloading regex-2025.7.34-cp312-cp312-win_amd64.whl (275 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading matplotlib-3.10.5-cp312-cp312-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   -------------------------------------- - 7.9/8.1 MB 40.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 35.8 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.3-cp312-cp312-win_amd64.whl (226 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.59.0-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 21.1 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.8-cp312-cp312-win_amd64.whl (71 kB)\n",
      "Downloading pillow-11.3.0-cp312-cp312-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 5.5/7.0 MB 25.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.0/7.0 MB 22.6 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: tqdm, regex, pyparsing, pillow, kiwisolver, ftfy, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.59.0 ftfy-6.3.1 kiwisolver-1.4.8 matplotlib-3.10.5 pillow-11.3.0 pyparsing-3.2.3 regex-2025.7.34 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\talgo\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gdown\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting beautifulsoup4 (from gdown)\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gdown) (3.17.0)\n",
      "Collecting requests[socks] (from gdown)\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gdown) (4.67.1)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->gdown)\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from beautifulsoup4->gdown) (4.12.2)\n",
      "Collecting charset_normalizer<4,>=2 (from requests[socks]->gdown)\n",
      "  Downloading charset_normalizer-3.4.2-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests[socks]->gdown)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests[socks]->gdown)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests[socks]->gdown)\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm->gdown) (0.4.6)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp312-cp312-win_amd64.whl (105 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: urllib3, soupsieve, PySocks, idna, charset_normalizer, certifi, requests, beautifulsoup4, gdown\n",
      "Successfully installed PySocks-1.7.1 beautifulsoup4-4.13.4 certifi-2025.8.3 charset_normalizer-3.4.2 gdown-5.2.0 idna-3.10 requests-2.32.4 soupsieve-2.7 urllib3-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\talgo\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\talgo\\appdata\\local\\temp\\pip-req-build-rxh8gjrr\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: ftfy in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from clip==1.0) (6.3.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from clip==1.0) (25.0)\n",
      "Requirement already satisfied: regex in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from clip==1.0) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from clip==1.0) (4.67.1)\n",
      "Requirement already satisfied: torch in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from clip==1.0) (2.6.0)\n",
      "Collecting torchvision (from clip==1.0)\n",
      "  Downloading torchvision-0.23.0-cp312-cp312-win_amd64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->clip==1.0) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->clip==1.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->clip==1.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->clip==1.0) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->clip==1.0) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->clip==1.0) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->clip==1.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torchvision->clip==1.0) (2.0.1)\n",
      "Collecting torch (from clip==1.0)\n",
      "  Downloading torch-2.8.0-cp312-cp312-win_amd64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torchvision->clip==1.0) (11.3.0)\n",
      "Collecting sympy>=1.13.3 (from torch->clip==1.0)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\talgo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
      "Downloading torchvision-0.23.0-cp312-cp312-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 0.8/1.6 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 10.7 MB/s eta 0:00:00\n",
      "Downloading torch-2.8.0-cp312-cp312-win_amd64.whl (241.3 MB)\n",
      "   ---------------------------------------- 0.0/241.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 8.4/241.3 MB 65.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 8.4/241.3 MB 65.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 9.2/241.3 MB 14.6 MB/s eta 0:00:16\n",
      "   - -------------------------------------- 11.3/241.3 MB 13.8 MB/s eta 0:00:17\n",
      "   -- ------------------------------------- 15.7/241.3 MB 15.2 MB/s eta 0:00:15\n",
      "   --- ------------------------------------ 18.4/241.3 MB 15.4 MB/s eta 0:00:15\n",
      "   --- ------------------------------------ 18.6/241.3 MB 13.2 MB/s eta 0:00:17\n",
      "   --- ------------------------------------ 19.4/241.3 MB 12.0 MB/s eta 0:00:19\n",
      "   --- ------------------------------------ 20.2/241.3 MB 10.8 MB/s eta 0:00:21\n",
      "   --- ------------------------------------ 21.5/241.3 MB 10.4 MB/s eta 0:00:22\n",
      "   --- ------------------------------------ 23.9/241.3 MB 10.5 MB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 26.2/241.3 MB 10.6 MB/s eta 0:00:21\n",
      "   ------ --------------------------------- 41.4/241.3 MB 15.3 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 62.9/241.3 MB 21.6 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 77.6/241.3 MB 25.2 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 80.7/241.3 MB 24.1 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 95.4/241.3 MB 26.9 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 96.2/241.3 MB 25.7 MB/s eta 0:00:06\n",
      "   ---------------- ---------------------- 100.9/241.3 MB 25.6 MB/s eta 0:00:06\n",
      "   ----------------- --------------------- 105.9/241.3 MB 25.3 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 123.5/241.3 MB 28.1 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 145.0/241.3 MB 31.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 164.6/241.3 MB 34.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 173.5/241.3 MB 34.4 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 195.3/241.3 MB 37.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 219.2/241.3 MB 40.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.3 MB 42.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.3 MB 42.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.3 MB 42.6 MB/s eta 0:00:01\n",
      "   --------------------------------------- 241.3/241.3 MB 38.6 MB/s eta 0:00:00\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.3/6.3 MB 77.4 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (pyproject.toml): started\n",
      "  Building wheel for clip (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369633 sha256=67cbbe525017144a6e3930df5a77741a7e6f02b6685ad094962a8601785cd8e0\n",
      "  Stored in directory: C:\\Users\\talgo\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-1a9u_0eh\\wheels\\35\\3e\\df\\3d24cbfb3b6a06f17a2bfd7d1138900d4365d9028aa8f6e92f\n",
      "Successfully built clip\n",
      "Installing collected packages: sympy, torch, torchvision, clip\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0\n",
      "    Uninstalling torch-2.6.0:\n",
      "      Successfully uninstalled torch-2.6.0\n",
      "Successfully installed clip-1.0 sympy-1.14.0 torch-2.8.0 torchvision-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\talgo\\AppData\\Local\\Temp\\pip-req-build-rxh8gjrr'\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\talgo\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%pip install ftfy regex tqdm scikit-learn numpy matplotlib\n",
    "%pip install -U gdown\n",
    "%pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32aa891",
   "metadata": {},
   "source": [
    "##### Data #####\n",
    "Now, we can import all of our relevant data! <br>\n",
    "For this project, we've created a drive folder, containing all of the relevant code and data from the original Pereira et al. paper. Moreover, because the list of concepts and the related images from the original paper is static, we pre-calculated all of the relevant CLIP embeddings (The exact code we used can be seen here in \"one_time_drive_setup\"), and persisted them to drive. Let's download all of the relevant data, so we could continue our analysis. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8345d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving folder 19K2VYTfEe1pyp4I0r5b9BJ-R0lQNKQVe brain-responses-data\n",
      "Processing file 1bIBosZxxCPhPvon3_ILDzYdVCVpRoUvR examples_180concepts_pictures.mat\n",
      "Processing file 1lYl9lo2SuM6QI89evn1b5vFAi2kmY-XG examples_180concepts_sentences.mat\n",
      "Processing file 1sFwuMcxYsO96I3mvMjUoHBpzqpulHkPK examples_180concepts_wordclouds.mat\n",
      "Processing file 12DkhAFbWbjJ8lbf92r-V1M2IszFCycUI clip_image_embeddings.npz\n",
      "Processing file 1GQbKpnB99I3InDqIo8PU_TT1Kk3GnizZ clip_text_embeddings.npz\n",
      "Processing file 1KVmlIBNinqCzOYOQJyq4M5oceHRojqUk concepts.txt\n",
      "Processing file 1i628lysbg60LZ9JN7l1cOq68GYWnxZ2S data_180concepts_sentences.mat\n",
      "Processing file 1Z0EsEpmzHkuGcDh9e7KSQGK8RlUlrLbF experiment-images.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talgo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gdown\\__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Retrieving folder contents\n",
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1bIBosZxxCPhPvon3_ILDzYdVCVpRoUvR\n",
      "From (redirected): https://drive.google.com/uc?id=1bIBosZxxCPhPvon3_ILDzYdVCVpRoUvR&confirm=t&uuid=92c486e0-ac62-4da7-a9db-c480542f579d\n",
      "To: c:\\Users\\talgo\\OneDrive\\Documents\\Technion baby\\2025ב\\שפה\\fmri-CLIP-decoder\\data\\brain-responses-data\\examples_180concepts_pictures.mat\n",
      "\n",
      "  0%|          | 0.00/287M [00:00<?, ?B/s]\n",
      "  0%|          | 524k/287M [00:00<01:12, 3.93MB/s]\n",
      "  1%|          | 2.10M/287M [00:00<00:31, 8.93MB/s]\n",
      "  2%|▏         | 6.29M/287M [00:00<00:12, 22.0MB/s]\n",
      "  4%|▍         | 11.5M/287M [00:00<00:08, 32.8MB/s]\n",
      "  5%|▌         | 15.2M/287M [00:00<00:08, 31.2MB/s]\n",
      "  8%|▊         | 22.5M/287M [00:00<00:06, 43.9MB/s]\n",
      " 10%|▉         | 27.3M/287M [00:00<00:07, 34.3MB/s]\n",
      " 12%|█▏        | 33.6M/287M [00:01<00:06, 40.2MB/s]\n",
      " 14%|█▍        | 41.4M/287M [00:01<00:05, 46.9MB/s]\n",
      " 16%|█▋        | 46.7M/287M [00:01<00:05, 42.5MB/s]\n",
      " 18%|█▊        | 53.0M/287M [00:01<00:04, 46.9MB/s]\n",
      " 21%|██        | 59.2M/287M [00:01<00:04, 50.4MB/s]\n",
      " 23%|██▎       | 65.5M/287M [00:01<00:05, 41.0MB/s]\n",
      " 25%|██▌       | 72.4M/287M [00:01<00:04, 46.5MB/s]\n",
      " 27%|██▋       | 78.6M/287M [00:01<00:04, 49.4MB/s]\n",
      " 30%|██▉       | 84.9M/287M [00:02<00:03, 52.7MB/s]\n",
      " 32%|███▏      | 90.7M/287M [00:02<00:03, 52.2MB/s]\n",
      " 34%|███▍      | 98.0M/287M [00:02<00:03, 54.4MB/s]\n",
      " 36%|███▌      | 104M/287M [00:02<00:04, 44.8MB/s] \n",
      " 39%|███▊      | 111M/287M [00:02<00:03, 48.2MB/s]\n",
      " 40%|████      | 116M/287M [00:02<00:03, 46.0MB/s]\n",
      " 42%|████▏     | 122M/287M [00:02<00:03, 48.4MB/s]\n",
      " 45%|████▌     | 129M/287M [00:02<00:03, 52.2MB/s]\n",
      " 47%|████▋     | 135M/287M [00:03<00:02, 51.1MB/s]\n",
      " 50%|████▉     | 142M/287M [00:03<00:02, 53.1MB/s]\n",
      " 52%|█████▏    | 148M/287M [00:03<00:02, 49.8MB/s]\n",
      " 54%|█████▍    | 155M/287M [00:03<00:02, 52.1MB/s]\n",
      " 57%|█████▋    | 163M/287M [00:03<00:02, 55.2MB/s]\n",
      " 59%|█████▉    | 170M/287M [00:03<00:01, 59.8MB/s]\n",
      " 61%|██████▏   | 176M/287M [00:03<00:02, 52.8MB/s]\n",
      " 63%|██████▎   | 182M/287M [00:04<00:02, 41.7MB/s]\n",
      " 66%|██████▌   | 190M/287M [00:04<00:02, 47.0MB/s]\n",
      " 69%|██████▊   | 197M/287M [00:04<00:01, 50.4MB/s]\n",
      " 71%|███████▏  | 205M/287M [00:04<00:01, 53.6MB/s]\n",
      " 74%|███████▍  | 212M/287M [00:04<00:01, 57.0MB/s]\n",
      " 76%|███████▌  | 218M/287M [00:04<00:01, 57.0MB/s]\n",
      " 78%|███████▊  | 224M/287M [00:04<00:01, 57.8MB/s]\n",
      " 81%|████████  | 231M/287M [00:04<00:00, 60.2MB/s]\n",
      " 83%|████████▎ | 238M/287M [00:04<00:00, 59.6MB/s]\n",
      " 85%|████████▍ | 244M/287M [00:05<00:00, 59.9MB/s]\n",
      " 87%|████████▋ | 251M/287M [00:05<00:00, 61.8MB/s]\n",
      " 90%|████████▉ | 257M/287M [00:05<00:00, 49.9MB/s]\n",
      " 92%|█████████▏| 263M/287M [00:05<00:00, 47.5MB/s]\n",
      " 93%|█████████▎| 268M/287M [00:05<00:00, 48.5MB/s]\n",
      " 96%|█████████▌| 276M/287M [00:05<00:00, 52.6MB/s]\n",
      " 99%|█████████▉| 284M/287M [00:05<00:00, 55.4MB/s]\n",
      "100%|██████████| 287M/287M [00:05<00:00, 49.1MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1lYl9lo2SuM6QI89evn1b5vFAi2kmY-XG\n",
      "From (redirected): https://drive.google.com/uc?id=1lYl9lo2SuM6QI89evn1b5vFAi2kmY-XG&confirm=t&uuid=a94ce0d5-0776-4972-8d26-8e9d6fbbeb03\n",
      "To: c:\\Users\\talgo\\OneDrive\\Documents\\Technion baby\\2025ב\\שפה\\fmri-CLIP-decoder\\data\\brain-responses-data\\examples_180concepts_sentences.mat\n",
      "\n",
      "  0%|          | 0.00/288M [00:00<?, ?B/s]\n",
      "  0%|          | 524k/288M [00:00<02:45, 1.74MB/s]\n",
      "  1%|          | 2.10M/288M [00:00<00:48, 5.85MB/s]\n",
      "  2%|▏         | 6.82M/288M [00:00<00:15, 18.1MB/s]\n",
      "  3%|▎         | 9.44M/288M [00:00<00:16, 17.0MB/s]\n",
      "  4%|▍         | 12.1M/288M [00:00<00:14, 19.0MB/s]\n",
      "  6%|▌         | 17.3M/288M [00:00<00:09, 27.6MB/s]\n",
      "  7%|▋         | 21.5M/288M [00:01<00:10, 24.4MB/s]\n",
      " 10%|▉         | 28.3M/288M [00:01<00:07, 34.1MB/s]\n",
      " 11%|█▏        | 32.5M/288M [00:01<00:08, 29.2MB/s]\n",
      " 13%|█▎        | 37.7M/288M [00:01<00:07, 34.1MB/s]\n",
      " 15%|█▌        | 44.0M/288M [00:01<00:06, 40.4MB/s]\n",
      " 18%|█▊        | 50.9M/288M [00:01<00:04, 47.4MB/s]\n",
      " 20%|█▉        | 56.6M/288M [00:01<00:04, 49.7MB/s]\n",
      " 22%|██▏       | 62.4M/288M [00:01<00:04, 50.2MB/s]\n",
      " 24%|██▎       | 68.2M/288M [00:02<00:04, 48.4MB/s]\n",
      " 26%|██▌       | 73.4M/288M [00:02<00:04, 49.4MB/s]\n",
      " 28%|██▊       | 79.2M/288M [00:02<00:04, 50.5MB/s]\n",
      " 29%|██▉       | 84.4M/288M [00:02<00:04, 50.5MB/s]\n",
      " 32%|███▏      | 91.2M/288M [00:02<00:03, 54.4MB/s]\n",
      " 34%|███▎      | 97.0M/288M [00:02<00:03, 55.3MB/s]\n",
      " 36%|███▌      | 103M/288M [00:02<00:03, 57.1MB/s] \n",
      " 38%|███▊      | 110M/288M [00:02<00:03, 57.3MB/s]\n",
      " 40%|████      | 116M/288M [00:02<00:02, 58.7MB/s]\n",
      " 43%|████▎     | 123M/288M [00:03<00:02, 59.8MB/s]\n",
      " 45%|████▌     | 129M/288M [00:03<00:02, 60.8MB/s]\n",
      " 47%|████▋     | 136M/288M [00:03<00:03, 48.5MB/s]\n",
      " 49%|████▉     | 142M/288M [00:03<00:02, 50.6MB/s]\n",
      " 51%|█████▏    | 148M/288M [00:03<00:02, 53.3MB/s]\n",
      " 54%|█████▍    | 155M/288M [00:03<00:02, 55.4MB/s]\n",
      " 56%|█████▌    | 161M/288M [00:03<00:02, 58.5MB/s]\n",
      " 58%|█████▊    | 168M/288M [00:03<00:02, 51.2MB/s]\n",
      " 61%|██████    | 175M/288M [00:04<00:02, 54.5MB/s]\n",
      " 63%|██████▎   | 180M/288M [00:04<00:01, 54.4MB/s]\n",
      " 65%|██████▍   | 187M/288M [00:04<00:01, 55.2MB/s]\n",
      " 67%|██████▋   | 192M/288M [00:04<00:02, 46.6MB/s]\n",
      " 69%|██████▉   | 199M/288M [00:04<00:01, 50.4MB/s]\n",
      " 71%|███████▏  | 206M/288M [00:04<00:01, 54.2MB/s]\n",
      " 74%|███████▎  | 212M/288M [00:04<00:01, 56.1MB/s]\n",
      " 76%|███████▌  | 219M/288M [00:04<00:01, 55.5MB/s]\n",
      " 78%|███████▊  | 224M/288M [00:04<00:01, 48.9MB/s]\n",
      " 81%|████████  | 232M/288M [00:05<00:01, 54.3MB/s]\n",
      " 83%|████████▎ | 238M/288M [00:05<00:01, 47.7MB/s]\n",
      " 84%|████████▍ | 243M/288M [00:05<00:00, 46.9MB/s]\n",
      " 86%|████████▋ | 249M/288M [00:05<00:00, 49.2MB/s]\n",
      " 89%|████████▉ | 256M/288M [00:05<00:00, 53.5MB/s]\n",
      " 92%|█████████▏| 264M/288M [00:05<00:00, 56.1MB/s]\n",
      " 94%|█████████▍| 271M/288M [00:05<00:00, 60.6MB/s]\n",
      " 96%|█████████▋| 277M/288M [00:05<00:00, 60.9MB/s]\n",
      " 99%|█████████▊| 284M/288M [00:06<00:00, 60.1MB/s]\n",
      "100%|██████████| 288M/288M [00:06<00:00, 47.3MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1sFwuMcxYsO96I3mvMjUoHBpzqpulHkPK\n",
      "From (redirected): https://drive.google.com/uc?id=1sFwuMcxYsO96I3mvMjUoHBpzqpulHkPK&confirm=t&uuid=ded01974-cff4-42d2-94d3-33ec08edabbb\n",
      "To: c:\\Users\\talgo\\OneDrive\\Documents\\Technion baby\\2025ב\\שפה\\fmri-CLIP-decoder\\data\\brain-responses-data\\examples_180concepts_wordclouds.mat\n",
      "\n",
      "  0%|          | 0.00/285M [00:00<?, ?B/s]\n",
      "  0%|          | 524k/285M [00:00<04:54, 965kB/s]\n",
      "  1%|          | 1.57M/285M [00:00<01:37, 2.90MB/s]\n",
      "  2%|▏         | 4.72M/285M [00:00<00:30, 9.32MB/s]\n",
      "  4%|▎         | 10.5M/285M [00:00<00:13, 21.0MB/s]\n",
      "  5%|▍         | 14.2M/285M [00:01<00:13, 19.9MB/s]\n",
      "  6%|▌         | 17.3M/285M [00:01<00:12, 22.2MB/s]\n",
      "  8%|▊         | 21.5M/285M [00:01<00:11, 23.2MB/s]\n",
      " 10%|▉         | 27.8M/285M [00:01<00:08, 32.0MB/s]\n",
      " 12%|█▏        | 33.6M/285M [00:01<00:06, 37.7MB/s]\n",
      " 14%|█▍        | 40.4M/285M [00:01<00:05, 42.8MB/s]\n",
      " 16%|█▌        | 45.1M/285M [00:02<00:09, 26.2MB/s]\n",
      " 18%|█▊        | 50.3M/285M [00:02<00:07, 30.8MB/s]\n",
      " 20%|██        | 58.2M/285M [00:02<00:05, 38.4MB/s]\n",
      " 23%|██▎       | 65.5M/285M [00:02<00:04, 43.9MB/s]\n",
      " 26%|██▌       | 72.9M/285M [00:02<00:04, 49.1MB/s]\n",
      " 28%|██▊       | 80.2M/285M [00:02<00:03, 54.7MB/s]\n",
      " 30%|███       | 86.5M/285M [00:02<00:04, 48.3MB/s]\n",
      " 33%|███▎      | 94.4M/285M [00:02<00:03, 52.3MB/s]\n",
      " 35%|███▌      | 100M/285M [00:03<00:04, 42.3MB/s] \n",
      " 37%|███▋      | 105M/285M [00:03<00:04, 38.2MB/s]\n",
      " 40%|███▉      | 113M/285M [00:03<00:03, 43.6MB/s]\n",
      " 42%|████▏     | 121M/285M [00:03<00:03, 48.5MB/s]\n",
      " 45%|████▌     | 128M/285M [00:03<00:02, 52.2MB/s]\n",
      " 48%|████▊     | 136M/285M [00:03<00:02, 54.5MB/s]\n",
      " 50%|█████     | 143M/285M [00:03<00:02, 57.7MB/s]\n",
      " 52%|█████▏    | 149M/285M [00:03<00:02, 56.9MB/s]\n",
      " 54%|█████▍    | 155M/285M [00:04<00:02, 52.5MB/s]\n",
      " 57%|█████▋    | 163M/285M [00:04<00:02, 55.5MB/s]\n",
      " 59%|█████▉    | 169M/285M [00:04<00:02, 47.5MB/s]\n",
      " 61%|██████    | 174M/285M [00:04<00:02, 48.5MB/s]\n",
      " 64%|██████▍   | 182M/285M [00:04<00:01, 52.5MB/s]\n",
      " 67%|██████▋   | 190M/285M [00:04<00:01, 55.5MB/s]\n",
      " 69%|██████▊   | 196M/285M [00:04<00:01, 53.0MB/s]\n",
      " 71%|███████   | 201M/285M [00:05<00:01, 53.3MB/s]\n",
      " 73%|███████▎  | 209M/285M [00:05<00:01, 55.3MB/s]\n",
      " 76%|███████▌  | 216M/285M [00:05<00:01, 56.9MB/s]\n",
      " 78%|███████▊  | 223M/285M [00:05<00:01, 61.0MB/s]\n",
      " 81%|████████  | 230M/285M [00:05<00:00, 59.4MB/s]\n",
      " 83%|████████▎ | 236M/285M [00:05<00:00, 49.4MB/s]\n",
      " 85%|████████▍ | 241M/285M [00:05<00:00, 46.5MB/s]\n",
      " 87%|████████▋ | 247M/285M [00:05<00:00, 48.9MB/s]\n",
      " 89%|████████▉ | 254M/285M [00:05<00:00, 53.4MB/s]\n",
      " 92%|█████████▏| 261M/285M [00:06<00:00, 56.9MB/s]\n",
      " 94%|█████████▍| 267M/285M [00:06<00:00, 56.5MB/s]\n",
      " 96%|█████████▋| 274M/285M [00:06<00:00, 59.5MB/s]\n",
      " 99%|█████████▊| 281M/285M [00:06<00:00, 59.0MB/s]\n",
      "100%|██████████| 285M/285M [00:06<00:00, 43.8MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=12DkhAFbWbjJ8lbf92r-V1M2IszFCycUI\n",
      "To: c:\\Users\\talgo\\OneDrive\\Documents\\Technion baby\\2025ב\\שפה\\fmri-CLIP-decoder\\data\\clip_image_embeddings.npz\n",
      "\n",
      "  0%|          | 0.00/3.09M [00:00<?, ?B/s]\n",
      "100%|██████████| 3.09M/3.09M [00:00<00:00, 25.8MB/s]\n",
      "100%|██████████| 3.09M/3.09M [00:00<00:00, 25.8MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1GQbKpnB99I3InDqIo8PU_TT1Kk3GnizZ\n",
      "To: c:\\Users\\talgo\\OneDrive\\Documents\\Technion baby\\2025ב\\שפה\\fmri-CLIP-decoder\\data\\clip_text_embeddings.npz\n",
      "\n",
      "  0%|          | 0.00/514k [00:00<?, ?B/s]\n",
      "100%|██████████| 514k/514k [00:00<00:00, 1.74MB/s]\n",
      "100%|██████████| 514k/514k [00:00<00:00, 1.74MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1KVmlIBNinqCzOYOQJyq4M5oceHRojqUk\n",
      "To: c:\\Users\\talgo\\OneDrive\\Documents\\Technion baby\\2025ב\\שפה\\fmri-CLIP-decoder\\data\\concepts.txt\n",
      "\n",
      "  0%|          | 0.00/1.30k [00:00<?, ?B/s]\n",
      "100%|██████████| 1.30k/1.30k [00:00<?, ?B/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1i628lysbg60LZ9JN7l1cOq68GYWnxZ2S\n",
      "From (redirected): https://drive.google.com/uc?id=1i628lysbg60LZ9JN7l1cOq68GYWnxZ2S&confirm=t&uuid=8b0a8702-aeaa-43a4-a0e4-9878f8da554f\n",
      "To: c:\\Users\\talgo\\OneDrive\\Documents\\Technion baby\\2025ב\\שפה\\fmri-CLIP-decoder\\data\\data_180concepts_sentences.mat\n",
      "\n",
      "  0%|          | 0.00/294M [00:00<?, ?B/s]\n",
      "  0%|          | 524k/294M [00:00<02:16, 2.15MB/s]\n",
      "  1%|          | 2.10M/294M [00:00<00:43, 6.64MB/s]\n",
      "  2%|▏         | 6.82M/294M [00:00<00:14, 19.7MB/s]\n",
      "  5%|▍         | 13.6M/294M [00:00<00:08, 34.3MB/s]\n",
      "  7%|▋         | 20.4M/294M [00:00<00:06, 43.9MB/s]\n",
      "  9%|▉         | 27.3M/294M [00:00<00:05, 50.6MB/s]\n",
      " 11%|█▏        | 33.6M/294M [00:00<00:04, 53.7MB/s]\n",
      " 14%|█▎        | 40.4M/294M [00:00<00:04, 56.9MB/s]\n",
      " 16%|█▌        | 47.2M/294M [00:01<00:04, 59.5MB/s]\n",
      " 18%|█▊        | 54.0M/294M [00:01<00:03, 60.1MB/s]\n",
      " 21%|██        | 60.8M/294M [00:01<00:03, 61.4MB/s]\n",
      " 23%|██▎       | 67.6M/294M [00:01<00:03, 62.4MB/s]\n",
      " 25%|██▌       | 74.4M/294M [00:01<00:03, 63.6MB/s]\n",
      " 28%|██▊       | 81.3M/294M [00:01<00:03, 62.7MB/s]\n",
      " 30%|██▉       | 88.1M/294M [00:01<00:03, 63.4MB/s]\n",
      " 32%|███▏      | 94.9M/294M [00:01<00:03, 64.1MB/s]\n",
      " 35%|███▍      | 102M/294M [00:01<00:02, 64.2MB/s] \n",
      " 37%|███▋      | 109M/294M [00:02<00:02, 64.1MB/s]\n",
      " 39%|███▉      | 115M/294M [00:02<00:02, 63.8MB/s]\n",
      " 42%|████▏     | 122M/294M [00:02<00:02, 64.0MB/s]\n",
      " 44%|████▍     | 129M/294M [00:02<00:02, 64.1MB/s]\n",
      " 46%|████▌     | 136M/294M [00:02<00:02, 63.9MB/s]\n",
      " 48%|████▊     | 143M/294M [00:02<00:02, 63.3MB/s]\n",
      " 51%|█████     | 150M/294M [00:02<00:02, 63.8MB/s]\n",
      " 53%|█████▎    | 157M/294M [00:02<00:02, 65.0MB/s]\n",
      " 56%|█████▌    | 164M/294M [00:02<00:02, 63.4MB/s]\n",
      " 58%|█████▊    | 170M/294M [00:03<00:01, 63.8MB/s]\n",
      " 60%|██████    | 177M/294M [00:03<00:01, 64.8MB/s]\n",
      " 63%|██████▎   | 184M/294M [00:03<00:01, 63.7MB/s]\n",
      " 65%|██████▍   | 191M/294M [00:03<00:01, 62.7MB/s]\n",
      " 67%|██████▋   | 198M/294M [00:03<00:01, 64.1MB/s]\n",
      " 70%|██████▉   | 204M/294M [00:03<00:01, 57.6MB/s]\n",
      " 72%|███████▏  | 212M/294M [00:03<00:01, 62.8MB/s]\n",
      " 75%|███████▍  | 220M/294M [00:03<00:01, 65.7MB/s]\n",
      " 77%|███████▋  | 226M/294M [00:03<00:01, 63.4MB/s]\n",
      " 79%|███████▉  | 233M/294M [00:04<00:00, 62.7MB/s]\n",
      " 82%|████████▏ | 240M/294M [00:04<00:00, 63.4MB/s]\n",
      " 84%|████████▍ | 248M/294M [00:04<00:00, 63.8MB/s]\n",
      " 87%|████████▋ | 255M/294M [00:04<00:00, 66.4MB/s]\n",
      " 89%|████████▉ | 262M/294M [00:04<00:00, 64.3MB/s]\n",
      " 91%|█████████▏| 269M/294M [00:04<00:00, 63.2MB/s]\n",
      " 94%|█████████▍| 276M/294M [00:04<00:00, 63.8MB/s]\n",
      " 96%|█████████▋| 283M/294M [00:04<00:00, 63.2MB/s]\n",
      " 99%|█████████▉| 291M/294M [00:04<00:00, 63.4MB/s]\n",
      "100%|██████████| 294M/294M [00:04<00:00, 59.2MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1Z0EsEpmzHkuGcDh9e7KSQGK8RlUlrLbF\n",
      "From (redirected): https://drive.google.com/uc?id=1Z0EsEpmzHkuGcDh9e7KSQGK8RlUlrLbF&confirm=t&uuid=403fb920-cbee-4a3c-8aca-1f1fe235fc64\n",
      "To: c:\\Users\\talgo\\OneDrive\\Documents\\Technion baby\\2025ב\\שפה\\fmri-CLIP-decoder\\data\\experiment-images.zip\n",
      "\n",
      "  0%|          | 0.00/110M [00:00<?, ?B/s]\n",
      "  0%|          | 524k/110M [00:00<01:03, 1.73MB/s]\n",
      "  2%|▏         | 2.10M/110M [00:00<00:18, 5.87MB/s]\n",
      "  6%|▌         | 6.82M/110M [00:00<00:05, 18.1MB/s]\n",
      " 10%|▉         | 10.5M/110M [00:00<00:04, 23.5MB/s]\n",
      " 12%|█▏        | 13.6M/110M [00:00<00:03, 25.5MB/s]\n",
      " 15%|█▌        | 16.8M/110M [00:00<00:03, 27.2MB/s]\n",
      " 20%|██        | 22.5M/110M [00:00<00:02, 35.6MB/s]\n",
      " 27%|██▋       | 29.9M/110M [00:01<00:01, 46.6MB/s]\n",
      " 32%|███▏      | 35.1M/110M [00:01<00:01, 47.0MB/s]\n",
      " 37%|███▋      | 40.9M/110M [00:01<00:01, 49.6MB/s]\n",
      " 42%|████▏     | 46.7M/110M [00:01<00:01, 51.4MB/s]\n",
      " 49%|████▊     | 53.5M/110M [00:01<00:01, 54.4MB/s]\n",
      " 56%|█████▌    | 61.3M/110M [00:01<00:00, 57.5MB/s]\n",
      " 61%|██████▏   | 67.6M/110M [00:01<00:00, 56.9MB/s]\n",
      " 68%|██████▊   | 75.0M/110M [00:01<00:00, 58.5MB/s]\n",
      " 75%|███████▌  | 82.8M/110M [00:01<00:00, 60.1MB/s]\n",
      " 82%|████████▏ | 90.7M/110M [00:02<00:00, 61.8MB/s]\n",
      " 89%|████████▉ | 98.0M/110M [00:02<00:00, 61.8MB/s]\n",
      " 96%|█████████▌| 106M/110M [00:02<00:00, 62.5MB/s] \n",
      "100%|██████████| 110M/110M [00:02<00:00, 46.4MB/s]\n",
      "Download completed\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "# If the data already exists, we don't need to download it again\n",
    "if not Path(\"data\").exists():\n",
    "    # Check operating system - handlind the data is done differently on Windows and Linux.\n",
    "    # This will allow us to run the code on Colab, locally, and on any other platform we may choose.\n",
    "    IS_WINDOWS = platform.system() == \"Windows\"\n",
    "    \n",
    "    if IS_WINDOWS:\n",
    "        !python -m gdown --folder --id 1CwmFOsYFnq6t33KAzpvw0gaOTQXbcozs -O ./data/\n",
    "        !powershell -NoProfile -Command \"Expand-Archive -Path ./data/experiment-images.zip -DestinationPath ./data/ -Force\"\n",
    "        !powershell -NoProfile -Command \"Remove-Item ./data/experiment-images.zip\"\n",
    "    else:\n",
    "        !gdown --folder --id 1CwmFOsYFnq6t33KAzpvw0gaOTQXbcozs --output ./data\n",
    "        !unzip ./data/experiment-images.zip\n",
    "        !rm ./data/experiment-images.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f59534",
   "metadata": {},
   "source": [
    "For the embeddings, we saved the following:\n",
    "* <u>Textual data</u> - we saved the embedding of the prompt \"A picture of {c}\", where c is the name of the relevant concept. The reason for that choice is that CLIP reacts very well to prompting, and that embedding is improving results over the non-prompt version.\n",
    "* <u>Visual data</u> - because each concept has 6 separate images describing it, we embedded all of them to CLIP's embedding space. Since this is not a singular vector, there are several approaches on how to parse the data, but the base embedding will be the same nonetheless, so we saved the embeddings for all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acc8be26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with np.load(\"data/clip_text_embeddings.npz\") as text_embeddings:\n",
    "    clip_text_embeddings = text_embeddings[\"data\"]\n",
    "\n",
    "with np.load(\"data/clip_image_embeddings.npz\") as image_embeddings:\n",
    "    clip_image_embeddings = image_embeddings[\"embeddings\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fbdfa0",
   "metadata": {},
   "source": [
    "#### Training ####\n",
    "As stated earlier, we are going to keep Pereira et al's method of learning a **linear** decoder, that would be able to generalize from the data it saw to unseen data, and thus capture deeper meaning than the training data itself. In the original paper, they managed to achieve results which are **much** better than chances, implying that a linear decoder is more than sufficient to capture meaning of textual-fMRI data when the data is projected onto an embedding space. <br>\n",
    "We have changed the embedding space from GloVe to CLIP, in order to see if textual-fMRI data can capture meaning of images as well, but we believe that adding extra complexity to the model will defeat our purpose. If our hypothesis is correct, then a linear decoder will be able to capture the meaning of the images in the same embedding space, without adding any non-linearity - a simple model should suffice, as much as is did for the same modality data. <br>\n",
    "For that reason, we are going to train our decoder in the same way that it was trained in the original paper:\n",
    "* For each participant's fMRI data - we're only going to take the top 5000 relevant `voxels`.\n",
    "* For the decoder - we're going to learn a simple ridge regression (using the same code)\n",
    "<br> \n",
    "Another important thing to remember is that we're only going to feed the function **textual** fMRI data, because our hypothesis states that from the same areas in the brain responsible for interpreting textual data can also give us insight on visual data. That means that we will only train our model on textual data, and withold any visual data for evaluation only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174aad21",
   "metadata": {},
   "source": [
    "First, let's define the function that will help us determine the top 5000 relevant voxels: <br>\n",
    "The fMRI data from the experiments consists of a big series of voxel each corresponding to the activation in a different area in the brain. The problem here is that most of the voxels are non-importent and are just noise which will reduce our model's accuracy. For that reason we will clean up the data and only use the 5000 most influencing voxels out of the 200,000 in the original fMRI data and we will be doing so using the select_top_voxels_indexes function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3bf99d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression\n",
    "import numpy as np\n",
    "\n",
    "def select_top_voxels_indices(fmri_data, semantic_vectors, num_voxels=5000):\n",
    "    f_scores = []\n",
    "    for i in range(semantic_vectors.shape[1]):\n",
    "        f, _ = f_regression(fmri_data, semantic_vectors[:, i])\n",
    "        f_scores.append(f)\n",
    "\n",
    "    f_scores = np.array(f_scores)\n",
    "    voxel_scores = np.max(f_scores, axis=0)\n",
    "    top_voxel_indices = np.argsort(voxel_scores)[-num_voxels:]\n",
    "\n",
    "    return top_voxel_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4ef844",
   "metadata": {},
   "source": [
    "Now, let's take the fMRI textual data from our data folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0bc4f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "mat = scipy.io.loadmat(\"data/brain-responses-data/examples_180concepts_wordclouds.mat\")\n",
    "fmri_text_data = mat[\"examples\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f5f9a0",
   "metadata": {},
   "source": [
    "and get only our top voxels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7b9fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_voxel_indices = select_top_voxels_indices(fmri_text_data, clip_text_embeddings)\n",
    "reduced_fmri_data = fmri_text_data[:, top_voxel_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815f6c1f",
   "metadata": {},
   "source": [
    "Finally, let's train our textual decoder. We'll use the function \"learn_decoder\", which we took directly from the original paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc27aa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" learn_decoder \"\"\"\n",
    "import sklearn.linear_model\n",
    "\n",
    "def learn_decoder(data, vectors):\n",
    "     \"\"\" Given data (a CxV matrix of V voxel activations per C concepts)\n",
    "     and vectors (a CxD matrix of D semantic dimensions per C concepts)\n",
    "     find a matrix M such that the dot product of M and a V-dimensional \n",
    "     data vector gives a D-dimensional decoded semantic vector. \n",
    "\n",
    "     The matrix M is learned using ridge regression:\n",
    "     https://en.wikipedia.org/wiki/Tikhonov_regularization\n",
    "     \"\"\"\n",
    "     ridge = sklearn.linear_model.RidgeCV(\n",
    "         alphas=[1, 10, .01, 100, .001, 1000, .0001, 10000, .00001, 100000, .000001, 1000000],\n",
    "         fit_intercept=False\n",
    "     )\n",
    "     ridge.fit(data, vectors)\n",
    "     return ridge.coef_.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f44bc1b",
   "metadata": {},
   "source": [
    "The training itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b7d1741",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = learn_decoder(reduced_fmri_data, clip_text_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2901909",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "For evaluation, we're going to have the following guiding principles:\n",
    "1. \"Distance\" between datapoints - \n",
    "\n",
    "\n",
    "1. Strategy - because quality fMRI data is extremely limited (Pereira is still the only open English dataset for fMRI single conept data, similary to what we're trying to model. There are some other alternatives - Allen 672 which is Chinese, Tuckute 2024, which is for full sentences only, six words each, each of them on no more than 16 participants), we don't have much data to work with. That means we want to base our decoder on all of the available data, scarce as it is. Because of that fact, we'll choose to evaluate the data using K-Fold Cross Validation - that way, we can train on every piece of data we have, and still evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dc3836",
   "metadata": {},
   "source": [
    "### Multimodal to Cross-Modality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8af8fc9",
   "metadata": {},
   "source": [
    "To attempt to disprove our initial hypothesis we will attempt to train a decoder on both the concepts and images fmri data and test it on the image embedding, it should have a much higher accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d883d28",
   "metadata": {},
   "source": [
    "#### Load Core Data (fMRI, concepts, CLIP targets)  \n",
    "First we load `concepts_fmri_data` / `pictures_fmri_data`: the fmri data from the first experiment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e1484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "def extract_data_from_mat_file(path):\n",
    "    return scipy.io.loadmat(path)[\"examples\"]\n",
    "\n",
    "concepts_fmri_data = extract_data_from_mat_file(\"data/brain-responses-data/examples_180concepts_wordclouds.mat\")\n",
    "pictures_fmri_data = extract_data_from_mat_file(\"data/brain-responses-data/examples_180concepts_wordclouds.mat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3472ad8f",
   "metadata": {},
   "source": [
    "#### Per-Concept Image Embedding Averages  \n",
    "Aggregates all image embeddings for each concept (~6 images) into a single CLIP vector by averaging and L2-normalizing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ea8836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def compute_average_clip_embeddings(images_dir, clip_image_embeddings):\n",
    "    \"\"\"\n",
    "    Compute the average CLIP image embedding for each concept folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    concept_folders = sorted(os.listdir(images_dir))\n",
    "\n",
    "    averaged_embeddings = []\n",
    "    embedding_index = 0  # pointer into clip_image_embeddings\n",
    "\n",
    "    for concept in concept_folders:\n",
    "        folder_path = os.path.join(images_dir, concept)\n",
    "\n",
    "        # Get all .jpg images in this concept folder\n",
    "        image_files = sorted([\n",
    "            f for f in os.listdir(folder_path)\n",
    "            if f.lower().endswith(\".jpg\")\n",
    "        ])\n",
    "\n",
    "        num_images = len(image_files)\n",
    "        if num_images == 0:\n",
    "            print(f\"⚠️ No images found for '{concept}', skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Get the embeddings of the images for the concept\n",
    "        concept_embeds = clip_image_embeddings[\n",
    "            embedding_index : embedding_index + num_images\n",
    "        ]\n",
    "        embedding_index += num_images\n",
    "\n",
    "        # Average of embeddings\n",
    "        avg_embed = concept_embeds.mean(axis=0)\n",
    "\n",
    "        # Normalization\n",
    "        norm = np.linalg.norm(avg_embed)\n",
    "        if norm > 0:\n",
    "            avg_embed /= norm\n",
    "\n",
    "        averaged_embeddings.append(avg_embed)\n",
    "\n",
    "    averaged_embeddings = np.array(averaged_embeddings, dtype=np.float32)\n",
    "    print(\"Averaged embeddings shape:\", averaged_embeddings.shape)\n",
    "\n",
    "    return averaged_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4323d257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged embeddings shape: (180, 768)\n"
     ]
    }
   ],
   "source": [
    "images_dir = \"data/experiment-images\"  # path to your images\n",
    "clip_image_embedding_averages = compute_average_clip_embeddings(\n",
    "    images_dir,\n",
    "    clip_image_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bfd3c4",
   "metadata": {},
   "source": [
    "#### Voxel Selection (Top-K per Modality)  \n",
    "Ranks voxels by max absolute correlation with CLIP targets:  \n",
    "\n",
    "- For concepts: fMRI (concepts) vs text embeddings.  \n",
    "- For pictures: fMRI (pictures) vs image averages.  \n",
    "\n",
    "Takes the top 5,000 indices from each list.  \n",
    "Creates two candidate voxel sets tuned to each modality.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c74fdeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5000\n"
     ]
    }
   ],
   "source": [
    "top_voxels_num = 5000  # top voxels per dataset\n",
    "\n",
    "idx_concepts  = select_top_voxels_indices(concepts_fmri_data,  clip_text_embeddings, top_voxels_num)\n",
    "idx_pictures  = select_top_voxels_indices(pictures_fmri_data,  clip_image_embedding_averages, top_voxels_num)\n",
    "\n",
    "print(len(idx_concepts), len(idx_pictures))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b92f8e0",
   "metadata": {},
   "source": [
    "#### Union of Voxel Sets (Collision-Aware)  \n",
    "Builds a union of voxel indices from both lists, preserving first appearance.  \n",
    "If a voxel appears in both lists (collision), averages the two fMRI columns:  \n",
    "\n",
    "- unique to concepts → `concepts_fmri_data[:, v]`  \n",
    "- unique to pictures → `pictures_fmri_data[:, v]`  \n",
    "- in both → `0.5 * (concepts[:, v] + pictures[:, v])`  \n",
    "\n",
    "Yields `combined_fmri_data` with shape (180, K_unique). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb30a57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_union_fmri_matrix(\n",
    "    idx_concepts: np.ndarray,\n",
    "    idx_pictures: np.ndarray,\n",
    "    concepts_fmri_data: np.ndarray,\n",
    "    pictures_fmri_data: np.ndarray,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Merge voxel indices from concepts and pictures, preserving first-seen order.\n",
    "    For indices present in BOTH lists, average their voxel columns from the two fMRI matrices.\n",
    "    For indices present in only one list, take that source's voxel column.\n",
    "    \"\"\"\n",
    "    # Basic sanity checks\n",
    "    assert concepts_fmri_data.shape[0] == pictures_fmri_data.shape[0], \\\n",
    "        \"Row mismatch: concepts_fmri_data and pictures_fmri_data must have same N (items).\"\n",
    "    N, Vc = concepts_fmri_data.shape\n",
    "    _, Vp = pictures_fmri_data.shape\n",
    "    assert Vc == Vp, \"Voxel count mismatch: concepts and pictures must have same number of voxel columns.\"\n",
    "\n",
    "    # Normalize inputs to int arrays\n",
    "    idx_concepts = np.asarray(idx_concepts, dtype=int)\n",
    "    idx_pictures = np.asarray(idx_pictures, dtype=int)\n",
    "\n",
    "    # Build ordered union and record sources for each voxel\n",
    "    sources_by_voxel: dict[int, set[str]] = {}\n",
    "    union_indices_list: list[int] = []\n",
    "\n",
    "    def _add_indices(indices, tag):\n",
    "        for v in indices:\n",
    "            v = int(v)\n",
    "            if v not in sources_by_voxel:\n",
    "                sources_by_voxel[v] = {tag}\n",
    "                union_indices_list.append(v)   # preserve first-seen order\n",
    "            else:\n",
    "                sources_by_voxel[v].add(tag)\n",
    "\n",
    "    _add_indices(idx_concepts, \"concepts\")\n",
    "    _add_indices(idx_pictures, \"pictures\")\n",
    "\n",
    "    union_indices = np.array(union_indices_list, dtype=int)\n",
    "    print(f\"Union voxel count: {union_indices.size}\")\n",
    "\n",
    "    # Build combined matrix with collision handling\n",
    "    combined_fmri_data = np.empty((N, union_indices.size), dtype=np.float32)\n",
    "\n",
    "    for j, v in enumerate(union_indices):\n",
    "        srcs = sources_by_voxel[v]\n",
    "        if \"concepts\" in srcs and \"pictures\" in srcs:\n",
    "            # Collision → average the columns from both sources\n",
    "            combined_fmri_data[:, j] = 0.5 * (concepts_fmri_data[:, v] + pictures_fmri_data[:, v])\n",
    "        elif \"concepts\" in srcs:\n",
    "            combined_fmri_data[:, j] = concepts_fmri_data[:, v]\n",
    "        else:\n",
    "            combined_fmri_data[:, j] = pictures_fmri_data[:, v]\n",
    "\n",
    "    print(\"combined fmri data shape:\", combined_fmri_data.shape)\n",
    "    return union_indices, combined_fmri_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b027433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union voxel count: 9740\n",
      "combined fmri data shape: (180, 9740)\n"
     ]
    }
   ],
   "source": [
    "union_indices, combined_fmri_data = build_union_fmri_matrix(\n",
    "    idx_concepts=idx_concepts,\n",
    "    idx_pictures=idx_pictures,\n",
    "    concepts_fmri_data=concepts_fmri_data,\n",
    "    pictures_fmri_data=pictures_fmri_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c880ace",
   "metadata": {},
   "source": [
    "#### Train Decoder: fMRI → CLIP Text Space  \n",
    "Trains a decoder mapping `combined_fmri_data` → CLIP text embeddings.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae792e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_decoder = learn_decoder(combined_fmri_data, clip_text_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
