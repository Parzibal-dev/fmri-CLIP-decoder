{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1f76404",
   "metadata": {},
   "source": [
    "Just a script to embed all of the textual and image data in CLIP and save the embeddings in the drive folder\n",
    "Requires the image data to be unzipped, and all .ini files do be deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aed2fd7",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81937104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\user\\appdata\\local\\temp\\pip-req-build-l71ucxe0\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: ftfy in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from clip==1.0) (6.3.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from clip==1.0) (24.2)\n",
      "Requirement already satisfied: regex in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from clip==1.0) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from clip==1.0) (4.67.1)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from clip==1.0) (2.7.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from clip==1.0) (0.22.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (2025.7.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torch->clip==1.0) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torchvision->clip==1.0) (2.3.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user\\onedrive - technion\\technion bois\\שפה חישוביות וקוגניציה\\עבודות\\fmri-clip-decoder\\.conda\\lib\\site-packages (from torchvision->clip==1.0) (11.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\user\\AppData\\Local\\Temp\\pip-req-build-l71ucxe0'\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3634030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- stdlib & deps ------------------------------------------------------\n",
    "import os, sys, numpy as np, torch\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import clip\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# ----- paths --------------------------------------------------------------\n",
    "drive_root = Path(r\"G:/\")\n",
    "PROJ_DIR   = drive_root / \".shortcut-targets-by-id/1CwmFOsYFnq6t33KAzpvw0gaOTQXbcozs/brain-decoder-files\"\n",
    "images_dir = PROJ_DIR / \"experiment-images\"\n",
    "\n",
    "text_npz   = PROJ_DIR / \"clip_text_embeddings.npz\"\n",
    "image_npz  = PROJ_DIR / \"clip_image_embeddings.npz\"   # new: raw vectors\n",
    "assert images_dir.is_dir(), f\"{images_dir} not found!\"\n",
    "\n",
    "# ----- load CLIP model ----------------------------------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-L/14@336px\", device=device)  # returns 768-d vectors\n",
    "model.eval()\n",
    "\n",
    "# ----- concept labels -----------------------------------------------------\n",
    "concepts = np.genfromtxt(PROJ_DIR / \"concepts.txt\", dtype=str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8061a044",
   "metadata": {},
   "source": [
    "### Embed textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5497e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    toks = clip.tokenize([f\"A photo of {c}\" for c in concepts]).to(device)\n",
    "    txt  = model.encode_text(toks)\n",
    "    txt  = txt / txt.norm(dim=-1, keepdim=True)\n",
    "\n",
    "np.savez_compressed(text_npz, data=txt.cpu().numpy().astype(np.float32))\n",
    "print(\"Text vectors saved to\", text_npz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a910b8",
   "metadata": {},
   "source": [
    "### Embed image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdbe96c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding images: 100%|██████████| 17/17 [36:01<00:00, 127.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image vectors saved to  G:\\.shortcut-targets-by-id\\1CwmFOsYFnq6t33KAzpvw0gaOTQXbcozs\\brain-decoder-files\\clip_image_embeddings.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "img_paths = [(con, p) for con in concepts for p in (images_dir / con).glob(\"*.jpg\")]\n",
    "\n",
    "BATCH = 64\n",
    "all_vecs, all_cons, all_files = [], [], []\n",
    "\n",
    "for i in tqdm(range(0, len(img_paths), BATCH), desc=\"encoding images\"):\n",
    "    batch_paths = img_paths[i : i + BATCH]\n",
    "\n",
    "    imgs = [preprocess(Image.open(p).convert(\"RGB\")) for _, p in batch_paths]\n",
    "    imgs = torch.stack(imgs).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vecs = model.encode_image(imgs).float()\n",
    "    vecs = vecs / vecs.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    all_vecs.append(vecs.cpu())\n",
    "    all_cons.extend([con for con, _ in batch_paths])\n",
    "    all_files.extend([p.name for _, p in batch_paths])\n",
    "\n",
    "embeddings = torch.cat(all_vecs).numpy().astype(np.float32)\n",
    "\n",
    "np.savez_compressed(\n",
    "    image_npz,\n",
    "    embeddings=embeddings,\n",
    "    concepts=np.array(all_cons),\n",
    "    filenames=np.array(all_files),\n",
    ")\n",
    "print(f\"Image vectors saved to \", image_npz)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
